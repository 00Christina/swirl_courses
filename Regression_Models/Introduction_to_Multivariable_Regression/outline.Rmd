# Outline

## Strictly speaking, we are already doing multivariable regression.

* Regression in one variable produced 2 coefficients, a slope and an intercept.
* The intercept is really the coefficient of a special regressor with value 1 for every sample.
* Show this:
```{r}
ones <- rep(1, nrow(galton))
lm(child ~ 0 + I(ones) + parent, galton)
```

## How to recover the intercept?

* Recall that you could find the slope by subtracting the mean from both parent and children and regressing without an intercept.
```{r}
slope <- coef(lm(I(child - mean(child)) ~ 0 + I(parent-mean(parent)), galton)) 
```

* Once you have the slope, how would you get the intercept?
* Well, child = intercept + slope*parent + noise, 
* And now you know the slope, so you can calculate child-slope*parent and
* child - slope*parent = intercept + noise
* So you can regress child - slope*parent against constant, 1.
* Show:
```{r}
intercept <- coef(lm(I(child-slope*parent) ~ 1, galton))
```

## Which is a special case of something else

* Subtracting the mean, to get the slope, then recovering the intercept as above is a special case of general method, sometimes called Gaussian elimination.
* The mean is the coefficient of regression against a constant regressor with value 1.
* child - mean(child) is actually the residual of a regression of child against the constant, 1.
* Show:
```{r}
fit <- lm(child ~ 0 +  ones, galton)
head(fitted(fit))
mean(galton$child)
```

## The general case

* The mean of a variable is the residual of its regression against the constant 1.
* so, the difference of a variable and its mean is the residual when the variable is regressed against 1.
* Subtracting the mean of each variable is the same as replacing each variable by the residual of its regression against 1.
* To generalize to an arbitrary regressor, x, replace each variable by its residual when regressed against x.
* First we'll eliminate the intercept, or the constant regressor.
* Next we pick another regressor and eliminate it.
* Eventually we are left with just one regressor and we know how to find its coefficient.
* Now we go back to the beginning and reduce the outcome. Now we have one less regressor and we begin again.

## Trees

* Volume ~ 1 + Girth + Height

* First eliminate the intercept

* Next eliminate Girth

* Regress Volume ~ Height for coefficient of Height

* Go back one step, reduce Height
