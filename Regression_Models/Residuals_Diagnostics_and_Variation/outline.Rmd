Initialization

```{r}
local({
  set.seed(13121098)
  n <- 50
  x <- rnorm(n, sd=.5)
  y <- x + rnorm(n, sd=.3)
  out1 <<- data.frame(y=c(5,y), x=c(0,x))
  out2 <<- data.frame(y=c(0,y), x=c(5,x))
})

```


Figure, outlier with minor influence
```{r}
local({
  fit.1 <- lm(y ~ x, out1[-1,])
  fit <- lm(y ~ x, out1)
  plot(c(-3, 6), c(-3, 6), type='n', xlab="x", ylab="y", main="Outlier with minor influence")
  points(y ~ x, out1, pch=21, bg="lightblue")
  points(out1[1,2], out1[1,1], cex = 2, bg = "darkorange", col = "black", pch = 21)
  text(out1[1,2], out1[1,1]+.25, "Outlier", pos=3)
  abline(fit.1, lwd=2)
  abline(fit, col='darkorange', lwd=4, lty=2)
  legend('bottomright', c("Fit omitting outlier", "Fit with outlier"), lty=1:2, col=c("black", "darkorange"), lwd=c(2,4))
})
```


Figure: outlier with major influence
```{r}
local({
  fit.1 <- lm(y ~ x, out2[-1,])
  fit <- lm(y ~ x, out2)
  plot(c(-3, 6), c(-3, 6), type='n', xlab="x", ylab="y", main="Outlier with major influence")
  points(y ~ x, out2, pch=21, bg="lightblue")
  points(out2[1,2], out2[1,1], cex = 2, bg = "darkorange", col = "black", pch = 21)
  text(out2[1,2], out2[1,1]+.25, "Outlier", pos=3)
  abline(fit.1, lwd=2)
  abline(fit, col='darkorange', lwd=4, lty=2)
  legend('bottomright', c("Fit omitting outlier", "Fit with outlier"), lty=1:2, col=c("black", "darkorange"), lwd=c(2,4))
})
```

Objectives: to illustrate the diagnostics available in plot.lm and ?influence.measures.

Dfbeta, as difference between full leave-one-out models.

* residuals against fitted values, which =1
* a Scale-Location plot of sqrt(| residuals |) against fitted values, which=3

Measures based on leaving out a sample (aka cross validation).

Hat value, aka leverage.

```{r}
local({
  fit.m1 <- lm(y ~ x, out2[-1,]) # fit omitting first sample
  yval.m1 <- predict(fit.m1, out2[1,]) # predict the first sample based on that fit
  res.m1 <- out2[1,"y"] - yval.m1 # calculate the residual of the prediction
  c(res.m1, resid(fit)[1]/(1-hatvalues(fit)[1])) # show Brian's equality is true
  c(hatvalues(fit)[1], 1-resid(fit)[1]/res.m1)
  })
```

Standardized residuals (which use hat values.)

```{r}
local({
  fit <- lm(y ~ x, out2)
  df.residual(fit)
  # [1] 49
  sqrt(deviance(fit)/49)
  # [1] 0.4965815
  resid(fit)[1]/0.4965815
  #        1 
  # -3.153734 
  rstandard(fit)[1]
  #        1 
  # -5.192816 
  -3.15/sqrt(1-h1)
  #        1 
  # -5.186668
  })
```

* a Normal Q-Q plot , which=1, uses standardized residuals.

Rstudent using leave-one-out

```{r}
local({
    fit <- lm(y ~ x, out2)
    h1 <- hatvalues(fit)[1]
    fit.m1 <- lm(y ~ x, out2[-1,]) # fit omitting first sample
    sigma2.m1 <- deviance(fit.m1)/df.residual(fit.m1)
    c(rstudent(fit)[1], resid(fit)[1]/sqrt(sigma2.m1*(1-h1)))
})
```


Cook's distance, [see this](https://en.wikipedia.org/wiki/Cook's_distance)

```{r}
local({
  fit <- lm(y ~ x, out2)
  E1 <- rstandard(fit)[1]
  h1 <- hatvalues(fit)[1]
  c(E1^2*h1/(1-h1)/2, cooks.distance(fit)[1])
  # Another way
  fit.m1 <- lm(y ~ x, out2[-1,]) # fit omitting first sample
  yval.m1 <- predict(fit.m1, out2[1,]) # predict the first sample based on that fit
  res.m1 <- out2[1,"y"] - yval.m1 # calculate the residual of the prediction
  # h1 <- 1-resid(fit)[1]/res.m1 => h1/(1-h1) == res.m1/resid(fit)[1] - 1
})
```

  * Cook's distances against hat values (aka leverage.) which=5

dffits from Rstudent and hat value: $diffits = rstudent \sqrt{\frac{h}{1-h}}$ [see this](https://en.wikipedia.org/wiki/DFFITS)


