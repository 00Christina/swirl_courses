# Outline for OLS

1. Explain and motivate OLS
 * A regression line fits the data best in the sense of "ordinary least squares." In the case of Galton's data, squared error means that we square the difference between a child's actual height and our estimate of it. Then we sum up these squares over all 928 samples. The regression line is that which minimizes this quantity.
 * Ordinary least squares criteria are convenient because they provide two equations which can be solved for the two unknowns, intercept and slope. The first equation says that the errors in our estimates, also known as "residuals," have mean zero. The second equation says that our residuals must be uncorrelated with our predictors, the parents' heights.

2. Verify that residuals have mean zero and are uncorrelated with parents' heights.
  * Explain that if `fit <- lm(child ~ parent, galton)`, then `fit` is a named list and `fit$residuals` are the OLS residuals. Verify that `mean(fit$residuals)` is essentially zero.  
  * Verify that `cov(fit$residuals, galton$parents)` is essentially zero.

3. Referring to the slides and videos, give a brief verbal explanation of the algebra.
  * The equations for the OLS intercept and slope are found by supposing a change is made to the intercept and/or slope. Squaring out the resulting expressions produces three summations. The first sum is the original squared error, before the slope and/or intercept were changed. The third is a sum of squares, hence would increase squared error provided the middle term were zero. The middle sum is guaranteed to be zero precisely when the two equations are satisfied.
  * The third sum totals the squared changes themselves. For instance, if we had changed the OLS fit's intercept by adding 2, the third sum would be the total of 928 4's.

4. Verify the algebra numerically
 * Make a function (or have it defined in initFile) which produces an estimate based on slope and intercept. Say, `est <- function(intercept, slope)intercept + slope*galton$parent`
 * Explain that the regression line's slope and intercept are in `fit$coef`, and extract them as `ols.slope` and `ols.intercept`.
 * Make a function (or have it defined in initFile) which calculates the squared error associated with a given slope and intercept: `sqe <- function(slope, intercept)sum( (est(slope, intercept)-galton$child)^2)`
 * Show, using several examples that (approximately) `sqe(ols.intercept+b, ols.slope+m) == sqe(ols.intercept, ols.slope) + sum( est(b, m)^2 )`

5. Explain that the variance in children's heights is the sum of the variance of the OLS estimate plus the variance of the residuals. This follows from the same two equations. It means the variance of the OLS estimate is always less than the variance of the data. Explain $R^2.$ 

6. Using some data set with multiple predictors, e.g., datasets::attenu, show the OLS residual has mean zero and is uncorrelated with each of the predictors. Show the $R^2$ property as well. 
