- Class: meta
  Course: Exploratory_Data_Analysis
  Lesson: K_Means_Clustering
  Author: Swirl Coders
  Type: Coursera
  Organization: Johns Hopkins Bloomberg School of Public Health
  Version: 2.2.0

- Class: text
  Output: "K_Means_Clustering. (Slides for this and other Data Science courses may be found at github https://github.com/DataScienceSpecialization/courses/. If you care to use them, they must be downloaded as a zip file and viewed locally. This lesson corresponds to 04_ExploratoryAnalysis/kmeansClustering.)"


- Class: text
  Output:  In this lesson we'll learn about K Means clustering, another simple way of examining and organizing multi-dimensional data. As with hierarchical clustering, this technique is  most useful in the early stages of analysis when you're trying to get an understanding of the data, e.g., finding some pattern or relationship between different factors or variables. 

- Class: text
  Output:  Since clustering organizes data points that are close into groups we can ask obvious questions such as "How do we define close?", "How do we group things?", and "How do we interpret the grouping?" 

- Class: figure
  Output: To give you an idea of what we're talking about, consider these random points we generated, familiar to you if you've already gone through the hierarchical clustering lesson. We'll use them to demonstrate K means clustering. We'll do this in several steps, but first we'll explain the general idea.
  Figure: ranPoints.R
  FigureType: new

- Class: text
  Output: This is a partioning approach which requires that you first guess how many clusters you have (or want). Once you fix this number, you randomly create a "centroid" (a phantom point) for each cluster and assign each point or observation in your dataset to the one centroid to which it is closest. Once each point is assigned a centroid, readjust the centroid's position by making it the average of the points assigned to it. 

- Class: text
  Output: Now you recalculate the distance of the observations to the centroids and reassign any, if necessary, to the centroid closest to them. Again, once the reassignments are done, readjust the positions of the centroids based on the new cluster membership. The process stops once you reach an iteration in which no adjustments are made. 

- Class: mult_question
  Output: As described, what does this process require?
  AnswerChoices:  A defined distance metric; A number of clusters; An initial guess as to cluster centroids; All of the others
  CorrectAnswer:  All of the others
  AnswerTests: omnitest(correctVal='All of the others')
  Hint: Which choice includes all the others.

- Class: mult_question
  Output: So K-Means Clustering requires some distance metric (say Euclidean), a hypothesized fixed number of clusters, and an initial guess as to cluster centroids. As described, what does this process produce?
  AnswerChoices:  A final estimate of cluster centroids; An assignment of each point to a cluster; All of the others
  CorrectAnswer:  All of the others
  AnswerTests: omnitest(correctVal='All of the others')
  Hint: Which choice includes all the others.

- Class: text
  Output: When it's finished K Means clustering returns a final position of each cluster's centroid as well as the assignment of each data point or observation to a cluster.
     
- Class: text
  Output: Now we'll step through this process using our random points as our data. The coordinates of these are stored in 2 vectors, x and y. We eyeball the display and guess that there are 3 clusters. We'll pick 3 positions of centroids, one for each cluster.
    
- Class: cmd_question
  Output:  We've created two 3-long vectors for you, cx and cy. These respectively hold the x- and y- coordinates for 3 proposed centroids. For convenience, we've also stored them in a 2 by 3 matrix cmat. The x coordinates are in the first row and the y coordinates in the second. Look at cmat now.
  CorrectAnswer: cmat
  AnswerTests: omnitest(correctExpr='cmat')
  Hint: Type cmat at the command prompt.

- Class: cmd_question
  Output:  The coordinates of these points are (1,2), (1.8,1) and (2.5,1.5). We'll add these centroids to the plot of our points. Do this by calling the R command points with 6 arguments. The first 2 are cx and cy, and the third is col set equal to the concatenation of 3 colors, "red", "orange", and "purple". The fourth argument is pch set equal to 3, the fifth is cex set equal to 2, and the final is lwd also set equal to 2.
  CorrectAnswer: points(cx,cy,col=c("red","orange","purple"),pch=3,cex=2,lwd=2)
  AnswerTests: omnitest(correctExpr='points(cx,cy,col=c("red","orange","purple"),pch=3,cex=2,lwd=2)')
  Hint: Type points(cx,cy,col=c("red","orange","purple"),pch=3,cex=2,lwd=2) at the command prompt.

- Class: text
  Output: We see the first centroid (1,2) is in red. The second (1.8,1), to the right and below the first, is orange, and the final centroid (2.5,1.5), the furthest to the right, is purple.

- Class: mult_question
  Output: Now we have to calculate distances between each point and every centroid. There are 12 data points and 3 centroids. How many distances do we have to calculate?
  AnswerChoices:  15; 36; 9; 108
  CorrectAnswer: 36
  AnswerTests: omnitest(correctVal='36')
  Hint: The distance between each point and one centroid means 12 distances have to be calculated for each centroid. This has to be done for all 3 centroids.

- Class: cmd_question
  Output:  We've written a function for you called mdist which takes 4 arguments. The vectors of data points (x and y) are the first two and the two vectors of centroid coordinates (cx and cy) are the last two. Call mdist now with these arguments. 
  CorrectAnswer: mdist(x,y,cx,cy)
  AnswerTests: omnitest(correctExpr='mdist(x,y,cx,cy)')
  Hint: Type mdist(x,y,cx,cy) at the command prompt.

- Class: cmd_question
  Output:  We've stored the resulting matrix of distances for you in the matrix distTmp. Look at it now.
  CorrectAnswer: distTmp
  AnswerTests: omnitest(correctExpr='distTmp')
  Hint: Type distTmp at the command prompt.

- Class: mult_question
  Output: Now that we have the distances we have to assign a cluster to each point. To do that we'll look at each column and ?
  AnswerChoices:  pick the minimum entry; pick the maximum entry; add up the 3 entries.
  CorrectAnswer:  pick the minimum entry
  AnswerTests: omnitest(correctVal='pick the minimum entry')
  Hint: We assign each point to the centroid closest to it.

- Class: mult_question
  Output: From the distTmp entries, which cluster would point 6 be assigned to?
  AnswerChoices:  1; 2; 3; none of the above
  CorrectAnswer:  3
  AnswerTests: omnitest(correctVal='3')
  Hint: Which row in column 6 has the lowest value?


- Class: cmd_question
  Output:  R has a handy function which.min which you can apply to ALL the columns of distTmp with one call. Simply call the R function apply with 3 arguments. The first is distTmp, the second is 2 meaning the columns of distTmp, and the third is which.min, the function you want to apply to the columns of distTmp. Try this now.
  CorrectAnswer: apply(distTmp,2,which.min)
  AnswerTests: omnitest(correctExpr='apply(distTmp,2,which.min)')
  Hint: Type apply(distTmp,2,which.min) at the command prompt.

- Class: text
  Output: You can see that you were right and the 6th entry is indeed 3 as you answered before. We see the first 3 entries were assigned to the second (orange) cluster and only 2 points (4 and 8) were assigned to the first (red) cluster.

- Class: cmd_question
  Output:  We've stored the vector of cluster colors ("red","orange","purple") in the array cols1 for you and we've also stored the cluster assignments in the array newClust. Let's color the 12 data points  according to their assignments. Again, use the command points with 5 arguments. The first 2 are x and y. The third is pch set to 19, the fourth is cex set to 2, and the last, col is set to cols1[newClust].
  CorrectAnswer: points(x,y,pch=19,cex=2,col=cols1[newClust])
  AnswerTests: omnitest(correctExpr='points(x,y,pch=19,cex=2,col=cols1[newClust])')
  Hint: Type points(x,y,pch=19,cex=2,col=cols1[newClust]) at the command prompt.

- Class: text
  Output: Now we have to recalculate our centroids so they are the average (center of gravity) of the cluster of points assigned to them. We have to do the x and y coordinates separately. We'll do the x coordinate first. Recall that the vectors x and y hold the respective coordinates of our 12 data points.

- Class: cmd_question
  Output: We can use the R function tapply which applies "a function over a ragged array". This means that every element of the array is assigned a factor and the function is applied to  subsets of the array (identified by the factor vector). This allows us to take advantage of the factor vector newClust we calculated. Call tapply now with 3 arguments, x (the data), newClust (the factor array), and mean (the function to apply). 
  CorrectAnswer: tapply(x,newClust,mean)
  AnswerTests: omnitest(correctExpr='tapply(x,newClust,mean)')
  Hint: Type tapply(x,newClust,mean) at the command prompt.

- Class: cmd_question
  Output: Repeat the call, except now apply it to the vector y instead of x.
  CorrectAnswer: tapply(y,newClust,mean)
  AnswerTests: omnitest(correctExpr='tapply(y,newClust,mean)')
  Hint: Type tapply(y,newClust,mean) at the command prompt.


- Class: cmd_question
  Output: Now that we have  new x and new y coordinates for the 3 centroids we can plot them. We've stored off the coordinates for you in variables newCx and newCy. Use the R command points with these as the first 2 arguments. In addition, use the arguments col set equal to cols1, pch equal to 8, cex equal to 2 and lwd also equal to 2.
  CorrectAnswer:points(newCx,newCy,col=cols1,pch=8,cex=2,lwd=2)
  AnswerTests: omnitest(correctExpr='points(newCx,newCy,col=cols1,pch=8,cex=2,lwd=2)')
  Hint: Type points(newCx,newCy,col=cols1,pch=8,cex=2,lwd=2) at the command prompt.

- Class: cmd_question
  Output: We see how the centroids have moved closer to their respective clusters. This is especially true of the orange cluster. Now call the distance function mdist with the 4 arguments x, y, newCx, and newCy. This will allow us to reassign the data points to new clusters if necessary.
  CorrectAnswer: mdist(x,y,newCx,newCy)
  AnswerTests: omnitest(correctExpr='mdist(x,y,newCx,newCy)')
  Hint: Type mdist(x,y,newCx,newCy) at the command prompt.

- Class: cmd_question
  Output: We've stored off the new matrix of distances in the matrix distTmp2. Look at it now.
  CorrectAnswer: distTmp2
  AnswerTests: omnitest(correctExpr='distTmp2')
  Hint: Type distTmp2 at the command prompt.

- Class: figure
  Output: Euclidean distance is what you learned about in high school algebra. Given two points on a plane, (x1,y1) and (x2,y2), the Euclidean distance is the square root of the sums of the squares of the distances between the two x-coordinates (x1-x2) and the two y-coordinates (y1-y2). You probably recognize this as an application of the Pythagorean theorem which yields the length of the hypotenuse of a right triangle.
  Figure: showEuclid.R
  FigureType: new

- Class: text
  Output: It shouldn't be hard to believe that this generalizes to more than two dimensions as shown in the formula at the bottom of the picture shown here. 

- Class: text
  Output: Euclidean distance is distance "as the crow flies". Many applications, however,  can't realistically use crow-flying distance. Cars, for instance, have to follow roads. 

- Class: figure
  Output:  In this case, we can use Manhattan or city block distance (also known as a taxicab metric). This picture, copied from http://en.wikipedia.org/wiki/Taxicab_geometry, shows what this means.
  Figure: showTaxi.R
  FigureType: new 

- Class: text
  Output: You want to travel from the point at the lower left to the one on the top right. The shortest distance is the Euclidean (the green line), but you're limited to the grid, so you have to follow a path similar to those shown in red, blue, or yellow. These all have the same length (12) which is the number of small gray segments covered by their paths.

- Class: text
  Output: More formally, Manhattan distance is the sum of the absolute values of the distances between each coordinate, so the distance between the points (x1,y1) and (x2,y2) is |x1-x2|+|y1-y2|. As with Euclidean distance, this too generalizes to more than 2 dimensions.

- Class: figure
  Output:  Now we'll go back to our random points. You might have noticed that these points don't really look randomly positioned, and in fact, they're not. They were actually generated as 3 distinct clusters. We've put the coordinates of these points in a data frame for you, called dataFrame. 
  Figure: ranPoints.R
  FigureType: new 

- Class: text
  Output:  We'll use this dataFrame to demonstrate an agglomerative (bottom-up) technique of hierarchical clustering and create a dendrogram. This is an abstract picture (or graph) which shows how the 12 points in our dataset cluster together. Two clusters (initially, these are points) that are close are connected with a line, We'll use Euclidean distance as our metric of closeness.

- Class: cmd_question
  Output:  Run the R command dist with the argument dataFrame to compute the distances between all pairs of these points. By default dist uses Euclidean distance as its metric, but other metrics such as Manhattan, are available. Just use the default.
  CorrectAnswer: dist(dataFrame)
  AnswerTests: omnitest(correctExpr='dist(dataFrame)')
  Hint: Type dist(dataFrame) at the command prompt.

- Class: text
  Output: You see that the output is a lower triangular matrix with rows numbered from 2 to 12 and columns numbered from 1 to 11. Entry (i,j) indicates the distance between points i and j. Clearly you need only a lower triangular matrix since the distance between points i and j equals that between j and i.

- Class: mult_question
  Output: From the output of dist, what is the minimum distance between two points?
  AnswerChoices:  0.0815; 0.08317; -0.0700; 0.1085
  CorrectAnswer:   0.0815
  AnswerTests: omnitest(correctVal='0.0815')
  Hint: Recall a previous question where points 5 and 6 looked close.

- Class: figure
  Output: So 0.0815 (units are unspecified) between points 5 and 6 is the shortest distance. We can put these points in a single cluster and look for another close pair of points. 
  Figure: cluster56.R
  FigureType: new 

- Class: mult_question
  Output: Looking at the picture, what would be another good pair of points to put in another cluster given that 5 and 6 are already clustered?
  AnswerChoices:  7 and the cluster containing 5 ad 6; 10 and 11; 1 and 4; 7 and 8
  CorrectAnswer:   10 and 11
  AnswerTests: omnitest(correctVal='10 and 11')
  Hint: Which of the choices looks closest on the picture?

- Class: figure
  Output: So 10 and 11 are another pair of points that would be in a second cluster. We'll start creating our dendrogram now. Here're the original plot and two beginning pieces of the dendrogram. 
  Figure: startDendro.R
  FigureType: new 

- Class: cmd_question
  Output:  We can keep going like this in the obvious way and pair up individual points, but as luck would have it, R provides a simple function which you can call which creates a dendrogram for you. It's called hclust() and takes as an argument the pairwise distance matrix which we looked at before. We've created this matrix for you in a variable called distxy. Run hclust now with distxy as its argument and put the result in the variable hc.
  CorrectAnswer: hc <- hclust(distxy)
  AnswerTests: expr_creates_var("hc"); omnitest(correctExpr='hc <- hclust(distxy)')
  Hint: Type hc <- hclust(distxy) at the command prompt.

- Class: cmd_question
  Output:  You're probably curious and want to see hc. Call the R function plot with one argument, hc.
  CorrectAnswer: plot(hc)
  AnswerTests: omnitest(correctExpr='plot(hc)')
  Hint: Type plot(hc) at the command prompt.

- Class: cmd_question
  Output:  Nice plot, right? R's plot conveniently labeled everything for you. The points we saw are the leaves at the bottom of the graph, 5 and 6 are connected, as are 10 and 11. Moreover, we see that the original 3 groupings of points are closest together as leaves on the picture. That's reassuring.  Now call plot again, this time with the argument as.dendrogram(hc). 
  CorrectAnswer: plot(as.dendrogram(hc))
  AnswerTests: omnitest(correctExpr='plot(as.dendrogram(hc))')
  Hint: Type plot(as.dendrogram(hc)) at the command prompt.

- Class: cmd_question
  Output: The essentials are the same, but the labels are missing and the leaves (original points) are all printed at the same level. Notice that the vertical heights of the lines and labeling of the scale on the left edge give some indication of distance. Use the R command abline to draw a horizontal blue line at 1.5 on this plot. Recall that this requires 2 arguments, h=1.5 and col="blue".  
  CorrectAnswer: abline(h=1.5,col="blue")
  AnswerTests: omnitest(correctExpr='abline(h=1.5,col="blue")')
  Hint: Type abline(h=1.5,col="blue") at the command prompt.

- Class: cmd_question
  Output: We see that this blue line intersects 3 vertical lines and this tells us that using the distance 1.5 (unspecified units) gives us 3 clusters (1 through 4), (9 through 12), and (5 through 8). We call this a "cut" of our dendrogram. Now cut the dendrogam by drawing a red horizontal line at .4. 
  CorrectAnswer: abline(h=.4,col="red")
  AnswerTests: omnitest(correctExpr='abline(h=.4,col="red")')
  Hint: Type abline(h=.4,col="red") at the command prompt.

- Class: cmd_question
  Output: How many clusters are there with a cut at this distance?
  CorrectAnswer: 5
  AnswerTests: equiv_val(5)
  Hint: How many vertical lines does this red line cross?

- Class: cmd_question
  Output: We see that by cutting at .4 we have 5 clusters, indicating that this distance is small enough to break up our original grouping of points. If we drew a horizontal line at .05, how many clusters would we get
  CorrectAnswer: 12
  AnswerTests: equiv_val(12)
  Hint: Recall that our shortest distance was around .08, so a distance smaller than that would make all the points their own private clusters.

- Class: cmd_question
  Output: Try it now (draw a horizontal line at .05) and make the line green.
  CorrectAnswer: abline(h=.05,col="green")
  AnswerTests: abline(h=.05,col="green")
  Hint: Type abline(h=.05,col="green") at the command prompt.

- Class: text
  Output: So the number of clusters in your data depends on where you draw the line! (We said there's a lot of flexibility here.) Now that we've seen the practice, let's go back to some "theory". Notice that the two original groupings, 5 through 8, and 9 through 12, are connected with a horizontal line near the top of the display. You're probably wondering how distances between clusters of points are measured. 

- Class: text
  Output: There are several ways to do this. We'll just mention two. The first is called complete linkage and it says that if you're trying to measure a distance between two clusters, take the greatest distance between the pairs of points in those two clusters. Obviously such pairs contain one point from each cluster.

- Class: figure
  Output: So if we were measuring the distance between the two clusters of points (1 through 4) and (5 through 8), using complete linkage as the metric we would use the distance between points 4 and 8 as the measure since this is the largest distance between the pairs of those groups. 
  Figure: complete.R
  FigureType: new 

- Class: figure
  Output: The distance between the two clusters of points (9 through 12) and (5 through 8), using complete linkage as the metric is the distance between points 11 and 8 since this is the largest distance between the pairs of those groups. 
  Figure: complete2.R
  FigureType: new 

- Class: figure
  Output: As luck would have it, the distance between the two clusters of points (9 through 12) and (1 through 4), using complete linkage as the metric is the distance between points 11 and 4. 
  Figure: complete3.R
  FigureType: new 

- Class: cmd_question
  Output:  We've created the dataframe dFsm for you containing these 3 points, 4, 8, and 11. Run dist on dFsm to see what the smallest distance between these 3 points is.
  CorrectAnswer: dist(dFsm)
  AnswerTests: omnitest(correctExpr='dist(dFsm)')
  Hint: Type dist(dFsm) at the command prompt.

- Class: figure
  Output: We see that the smallest distance is between points 2 and 3 in this reduced set, (these are actually points 8 and 11 in the original set), indicating that the two clusters these points represent ((5 through 8) and (9 through 12) respectively) would be joined (at a distance of 1.869) before being connected with the third cluster (1 through 4). This is consistent with the dendrogram we plotted.
  Figure: dendro.R
  FigureType: new 

- Class: figure
  Output: The second way to measure a distance between two clusters that we'll just mention is called average linkage. First you compute an "average" point in each cluster (think of it as the cluster's center of gravity). You do this by computing the mean (average) x and y coordinates of the points in the cluster. 
  Figure: average.R
  FigureType: new 

- Class: figure
  Output: Then you compute the distances between each cluster average to compute the intercluster distance.
  Figure: average2.R
  FigureType: new 

- Class: cmd_question
  Output:  Now look at the hierarchical cluster we created before, hc.
  CorrectAnswer: hc
  AnswerTests: omnitest(correctExpr='hc')
  Hint: Type hc at the command prompt.

- Class: mult_question
  Output: Which type of linkage did hclust() use to agglomerate clusters?
  AnswerChoices:  average; complete
  CorrectAnswer:  complete
  AnswerTests: omnitest(correctVal='complete')
  Hint: Look at the output when you looked at hc. What was the cluster method?

- Class: text
  Output: In our simple set of data, the average and complete linkages aren't that different, but in more complicated datasets the type of linkage  you use could affect how your data clusters. It is a good idea to experiment with different methods of linkage to see the varying ways your data groups. This will help you determine the best way to continue with your analysis.

- Class: text
  Output: The last method of visualizing data we'll mention in this lesson concerns heat maps. Wikipedia (http://en.wikipedia.org/wiki/Heat_map) tells us a heat map is "a graphical representation of data where the individual values contained in a matrix are represented as colors. ... Heat maps originated in 2D displays of the values in a data matrix. Larger values were represented by small dark gray or black squares (pixels) and smaller values by lighter squares." 

- Class: text
  Output:  You've probably seen many examples of heat maps, for instance weather radar and displays of ocean salinity. From Wikipedia (http://en.wikipedia.org/wiki/Heat_map) we learn that heat maps are often used in molecular biology "to represent the level of expression of many genes across a number of comparable samples  (e.g. cells in different states, samples from different patients) as they are obtained from DNA microarrays."

- Class: figure
  Output: We won't say too much on this topic, but a very nice concise tutorial on creating heatmaps in R exists at  http://sebastianraschka.com/Articles/heatmaps_in_r.html#clustering. Here's an image from the tutorial to start you thinking about the underlying details.
  Figure: showheat.R
  FigureType: new 


- Class: cmd_question
  Output:  R provides a handy function to produce heat maps. It's called heatmap. We've put the point data we've been using throughout this lesson in a matrix. Call heatmap now with 2 arguments. The first is dataMatrix and the second is col set equal to cm.colors(25). This last is optional, but we like the colors better than the default ones.
  CorrectAnswer: heatmap(dataMatrix,col=cm.colors(25))
  AnswerTests: omnitest(correctExpr='heatmap(dataMatrix,col=cm.colors(25))')
  Hint: Type heatmap(dataMatrix,col=cm.colors(25)) at the command prompt.

- Class: text
  Output: We see an interesting display of sorts. This is a very simple heat map - simple because the data isn't very complex. The rows and columns are grouped together as shown by colors. The top rows (labeled 5, 6, and 7) seem to be in the same group (same colors) while 8 is next to them but colored differently. This matches the dendrogram shown on the left edge. Similarly, 9, 12, 11, and 10 are grouped together (row-wise) along with 3 and 2. These are followed by 1 and 4 which are in a separate group. Column data is treated independently of rows but is also grouped.

- Class: cmd_question
  Output:  We've subsetted some vehicle data from mtcars, the Motor Trend Car Road Tests which is part of the package datasets. The data is in the matrix mt and contains 6 factors of 11 cars. Run heatmap now with mt as its argument.
  CorrectAnswer: heatmap(mt)
  AnswerTests: omnitest(correctExpr='heatmap(mt)')
  Hint: Type heatmap(mt) at the command prompt.

- Class: cmd_question
  Output:  This looks slightly more interesting than the heatmap for the point data. It shows a little better how the rows and columns are treated (clustered and colored) independently of one another. To understand this better, call plot with one argument, the dendrogram object denmt we've created for you.
  CorrectAnswer: plot(denmt)
  AnswerTests: omnitest(correctExpr='plot(denmt)')
  Hint: Type plot(denmt) at the command prompt.

- Class: cmd_question
  Output:  To make more sense of this, recall that we generalized the distance formula for more than 2 dimensions. We've created a distance matrix for you, distmt. Look at it now.
  CorrectAnswer: distmt
  AnswerTests: omnitest(correctExpr='distmt')
  Hint: Type distmt at the command prompt.

- Class: text
  Output: See how these distances match those in the dendrogram?  So hclust really works! Let's review now.

- Class: mult_question
  Output:  What is the purpose of hierarchical clustering?
  AnswerChoices:  Give an idea of the relationships between variables or observations; Present a finished picture; Inspire other researchers; None of the others
  CorrectAnswer: Give an idea of the relationships between variables or observations
  AnswerTests: omnitest(correctVal='Give an idea of the relationships between variables or observations')
  Hint: Recall that this is a technique for EXPLORING data when you're first starting to research a problem.

- Class: mult_question
  Output:  True or False? When you're doing hierarchical clustering there are strict rules that you MUST follow.
  AnswerChoices:  True; False
  CorrectAnswer: False
  AnswerTests: omnitest(correctVal='False')
  Hint: There are always choices to be made in this area.

- Class: mult_question
  Output:  True or False? There's only one way to measure distance.
  AnswerChoices:  True; False
  CorrectAnswer: False
  AnswerTests: omnitest(correctVal='False')
  Hint: There are always choices to be made in this area.

- Class: mult_question
  Output:  True or False? Complete linkage is a method of computing distances between clusters.
  AnswerChoices:  True; False
  CorrectAnswer: True
  AnswerTests: omnitest(correctVal='True')
  Hint: Once a cluster contains more than one point you need a method of defining a distance between it and other clusters.

- Class: mult_question
  Output:  True or False? Average linkage uses the maximum distance between points of two clusters as the distance between those clusters.
  AnswerChoices:  True; False
  CorrectAnswer: False
  AnswerTests: omnitest(correctVal='False')
  Hint: Average linkage uses the average or mean point as the representative of a cluster. The distance between these average points is the distance between the clusters.

- Class: mult_question
  Output:  True or False? The number of clusters you derive from your data depends on the distance at which you choose to cut it.
  AnswerChoices:  True; False
  CorrectAnswer: True
  AnswerTests: omnitest(correctVal='True')
  Hint: Recall our example where we drew horizontal cut lines through the dendrogram.

- Class: mult_question
  Output:  True or False? Once you decide basics, such as defining a distance metric and linkage method, hierarchical clustering is deterministic.
  AnswerChoices:  True; False
  CorrectAnswer: True
  AnswerTests: omnitest(correctVal='True')
  Hint: Once you pick your algorithm, all you have to do is apply it.

- Class: text
  Output: Congratulations! We hope this lesson didn't fluster you!

