- Class: meta
  Course: Exploratory_Data_Analysis
  Lesson: Hierarchical_Clustering
  Author: Swirl Coders
  Type: Coursera
  Organization: Johns Hopkins Bloomberg School of Public Health
  Version: 2.2.0

- Class: text
  Output: "Hierarchical_Clustering. (Slides for this and other Data Science courses may be found at github https://github.com/DataScienceSpecialization/courses/. If you care to use them, they must be downloaded as a zip file and viewed locally. This lesson corresponds to 04_ExploratoryAnalysis/hierarchicalClustering.)"


- Class: text
  Output:  In this lesson we'll learn about hierarchical clustering, a simple way of quickly examining and displaying multi-dimensional data. This technique is usually most useful in the early stages of analysis when you're trying to get an understanding of the data, e.g., finding some pattern or relationship between different factors or variables. As the name suggests hierarchical clustering creates a hierarchy of clusters.

- Class: text
  Output:  Clustering organizes data points that are close into groups. So obvious questions are "How do we define close?", "How do we group things?", and "How do we interpret the grouping?" Cluster analysis is a very important topic in data analysis.

- Class: figure
  Output: To give you an idea of what we're talking about, consider these random points we generated. We'll use them to demonstrate a clustering technique in this lesson. We'll do this in several steps, but first we have to clarify our terms and concepts.
  Figure: ranPoints.R
  FigureType: new

- Class: text
  Output: We'll use an agglomerative, or bottom-up, approach. From Wikipedia  (http://en.wikipedia.org/wiki/Hierarchical_clustering), we learn that in this method, "each observation starts in its own cluster, and pairs of clusters are merged as one moves up the hierarchy." This means that we'll find the closest two points and put them together in one cluster, then find the next closest pair in the updated picture, and so forth. We'll repeat this process until we reach a reasonable stopping place.

- Class: text
  Output: Note the word "reasonable". There's a lot of flexibility in this field and how you perform your analysis depends on your problem. Again, Wikipedia tells us, "one can decide to stop clustering either when the clusters are too far apart to be merged (distance criterion) or when there is a sufficiently small number of clusters (number criterion)."

- Class: text
  Output: First, how do we define close? This is the most important step and there are several possibilities depending on the questions you're trying to answer and the data you have. Distance or similarity are usually the metrics used. 
     
- Class: mult_question
  Output: In the given plot which pair points would you first cluster? Use distance as the metric.
  AnswerChoices:  7 and 8; 1 and 4; 5 and 6; 10 and 12
  CorrectAnswer:  5 and 6
  AnswerTests: omnitest(correctVal='5 and 6')
  Hint: The choices aren't very close. Look at the picture for the answer.

- Class: text
  Output: It's pretty obvious that out of the 4 choices, the pair 5 and 6 were the closest together. However, there are several ways to measure distance or similarity. Euclidean distance and correlation similarity are continuous measures, while Manhattan distance is a binary measure. In this lesson we'll just briefly discuss the first and last of these. It's important that you use a measure of distance that fits your problem.

- Class: figure
  Output: Euclidean distance is what you learned about in high school algebra. Given two points on a plane, (x1,y1) and (x2,y2), the Euclidean distance is the square root of the sums of the squares of the distances between the two x-coordinates (x1-x2) and the two y-coordinates (y1-y2). You probably recognize this as an application of the Pythagorean theorem which yields the length of the hypotenuse of a right triangle.
  Figure: showEuclid.R
  FigureType: new

- Class: text
  Output: It shouldn't be hard to believe that this generalizes to more than two dimensions as shown in the formula at the bottom of the picture shown here. 

- Class: text
  Output: Euclidean distance is distance "as the crow flies". Many applications, however,  can't realistically use crow-flying distance. Cars, for instance, have to follow roads. 

- Class: figure
  Output:  In this case, we can use Manhattan or city block distance (also known as a taxicab metric). This picture, copied from http://en.wikipedia.org/wiki/Taxicab_geometry, shows what this means.
  Figure: showTaxi.R
  FigureType: new 

- Class: text
  Output: You want to travel from the point at the lower left to the one on the top right. The shortest distance is the Euclidean (the green line), but you're limited to the grid, so you have to follow a path similar to those shown in red, blue, or yellow. These all have the same length (12) which is the number of small gray segments covered by their paths.

- Class: text
  Output: More formally, Manhattan distance is the sum of the absolute values of the distances between each coordinate, so the distance between the points (x1,y1) and (x2,y2) is |x1-x2|+|y1-y2|. As with Euclidean distance, this too generalizes to more than 2 dimensions.

- Class: figure
  Output:  Now we'll go back to our random points. You might have noticed that these points don't really look randomly positioned, and in fact, they're not. They were actually generated as 3 distinct clusters. We've put the coordinates of these points in a data frame for you, called dataFrame. 
  Figure: ranPoints.R
  FigureType: new 

- Class: text
  Output:  We'll use this dataFrame to demonstrate an agglomerative (bottom-up) technique of hierarchical clustering and create a dendrogram. This is an abstract picture (or graph) which shows how the 12 points in our dataset cluster together. Two clusters (initially, these are points) that are close are connected with a line, We'll use Euclidean distance as our metric of closeness.

- Class: cmd_question
  Output:  Run the R command dist with the argument dataFrame to compute the distances between all pairs of these points. By default dist uses Euclidean distance as its metric, but other metrics such as Manhattan, are available. Just use the default.
  CorrectAnswer: dist(dataFrame)
  AnswerTests: omnitest(correctExpr='dist(dataFrame)')
  Hint: Type dist(dataFrame) at the command prompt.

- Class: text
  Output: You see that the output is a lower triangular matrix with rows numbered from 2 to 12 and columns numbered from 1 to 11. Entry (i,j) indicates the distance between points i and j. Clearly you need only a lower triangular matrix since the distance between points i and j equals that between j and i.

- Class: mult_question
  Output: From the output of dist, what is the minimum distance between two points?
  AnswerChoices:  0.0815; 0.08317; -0.0700; 0.1085
  CorrectAnswer:   0.0815
  AnswerTests: omnitest(correctVal='0.0815')
  Hint: Recall a previous question where points 5 and 6 looked close.

- Class: figure
  Output: So 0.0815 (units are unspecified) between points 5 and 6 is the shortest distance. We can put these points in a single cluster and look for another close pair of points. 
  Figure: cluster56.R
  FigureType: new 

- Class: mult_question
  Output: Looking at the picture, what would be another good pair of points to put in another cluster given that 5 and 6 are already clustered?
  AnswerChoices:  7 and the cluster containing 5 ad 6; 10 and 11; 1 and 4; 7 and 8
  CorrectAnswer:   10 and 11
  AnswerTests: omnitest(correctVal='10 and 11')
  Hint: Which of the choices looks closest on the picture?

- Class: figure
  Output: So 10 and 11 are another pair of points that would be in a second cluster. We'll start creating our dendrogram now. Here're the original plot and two beginning pieces of the dendrogram. 
  Figure: startDendro.R
  FigureType: new 

- Class: cmd_question
  Output:  We can keep going like this in the obvious way and pair up individual points, but as luck would have it, R provides a simple function which you can call which creates a dendrogram for you. It's called hclust() and takes as an argument the pairwise distance matrix which we looked at before. We've created this matrix for you in a variable called distxy. Run hclust now with distxy as its argument and put the result in the variable hc.
  CorrectAnswer: hc <- hclust(distxy)
  AnswerTests: expr_creates_var("hc"); omnitest(correctExpr='hc <- hclust(distxy)')
  Hint: Type hc <- hclust(distxy) at the command prompt.

- Class: cmd_question
  Output:  You're probably curious and want to see hc. Call the R function plot with one argument, hc.
  CorrectAnswer: plot(hc)
  AnswerTests: omnitest(correctExpr='plot(hc)')
  Hint: Type plot(hc) at the command prompt.

- Class: cmd_question
  Output:  Nice plot, right? R's plot conveniently labeled everything for you. The points we saw are the leaves at the bottom of the graph, 5 and 6 are connected, as are 10 and 11. Moreover, we see that the original 3 groupings of points are closest together as leaves on the picture. That's reassuring.  Now call plot again, this time with the argument as.dendrogram(hc). 
  CorrectAnswer: plot(as.dendrogram(hc))
  AnswerTests: omnitest(correctExpr='plot(as.dendrogram(hc))')
  Hint: Type plot(as.dendrogram(hc)) at the command prompt.

- Class: cmd_question
  Output: The essentials are the same, but the labels are missing and the leaves (original points) are all printed at the same level. Notice that the vertical heights of the lines and labeling of the scale on the left edge give some indication of distance. Use the R command abline to draw a horizontal blue line at 1.5 on this plot. Recall that this requires 2 arguments, h=1.5 and col="blue".  
  CorrectAnswer: abline(h=1.5,col="blue")
  AnswerTests: omnitest(correctExpr='abline(h=1.5,col="blue")')
  Hint: Type abline(h=1.5,col="blue") at the command prompt.

- Class: cmd_question
  Output: We see that this blue line intersects 3 vertical lines and this tells us that using the distance 1.5 (unspecified units) gives us 3 clusters (1 through 4), (9 through 12), and (5 through 8). Now draw a red horizontal line at .4. 
  CorrectAnswer: abline(h=.4,col="red")
  AnswerTests: omnitest(correctExpr='abline(h=.4,col="red")')
  Hint: Type abline(h=.4,col="red") at the command prompt.

- Class: cmd_question
  Output: How many clusters are there at this distance?
  CorrectAnswer: 5
  AnswerTests: equiv_val(5)
  Hint: How many vertical lines does this red line cross?

- Class: cmd_question
  Output: We see that at the distance .4 we have 5 clusters, indicating that this distance is small enough to break up our original grouping of points. If we drew a horizontal line at .05, how many clusters would we get
  CorrectAnswer: 12
  AnswerTests: equiv_val(12)
  Hint: Recall that our shortest distance was around .08, so a distance smaller than that would make all the points their own private clusters.

- Class: cmd_question
  Output: Try it now (draw a horizontal line at .05) and make the line green.
  CorrectAnswer: abline(h=.05,col="green")
  AnswerTests: abline(h=.05,col="green")
  Hint: Type abline(h=.05,col="green") at the command prompt.

- Class: text
  Output: So the number of clusters in your data depends on where you draw the line! (We said there's a lot of flexibility here.) Now that we've seen the practice, let's go back to some "theory". Notice that the two original groupings, 5 through 8, and 9 through 12, are connected with a horizontal line near the top of the display. You're probably wondering how distances between clusters of points are measured. 

- Class: text
  Output: There are several ways to do this. We'll just mention two. The first is called complete linkage and it says that if you're trying to measure a distance between two clusters, take the greatest distance between the pairs of points in those two clusters. Obviously such pairs contain one point from each cluster.

- Class: figure
  Output: So if we were measuring the distance between the two clusters of points (1 through 4) and (5 through 8), using complete linkage as the metric we would use the distance between points 4 and 8 as the measure since this is the largest distance between the pairs of those groups. 
  Figure: complete.R
  FigureType: new 

- Class: figure
  Output: The distance between the two clusters of points (9 through 12) and (5 through 8), using complete linkage as the metric is the distance between points 11 and 8 since this is the largest distance between the pairs of those groups. 
  Figure: complete2.R
  FigureType: new 

- Class: figure
  Output: As luck would have it, the distance between the two clusters of points (9 through 12) and (1 through 4), using complete linkage as the metric is the distance between points 11 and 4. 
  Figure: complete3.R
  FigureType: new 

- Class: cmd_question
  Output:  We've created the dataframe dFsm for you containing these 3 points, 4, 8, and 11. Run dist on dFsm to see what the smallest distance between these 3 points is.
  CorrectAnswer: dist(dFsm)
  AnswerTests: omnitest(correctExpr='dist(dFsm)')
  Hint: Type dist(dFsm) at the command prompt.

- Class: figure
  Output: We see that the smallest distance is between points 2 and 3 in this reduced set, actually points 8 and 11 in the original set, indicated that the two clusters these points represents (5 through 8) and (9 through 12) respectively would be joined (at a distance of 1.869) before being connected with the third cluster (1 through 4). This is consistent with the dendrogram we plotted.
  Figure: dendro.R
  FigureType: new 


- Class: figure
  Output: The second way to measure a distance between two clusters that we'll just mention is called average linkage. First you compute an "average" point in each cluster (think of it as a center of gravity). You do this by computing the mean x and y coordinates. 
  Figure: average.R
  FigureType: new 

- Class: figure
  Output: Then you compute the distances between each cluster average to compute the intercluster distance.
  Figure: average2.R
  FigureType: new 

- Class: cmd_question
  Output:  Now look at the hierarchical cluster we created before, hc.
  CorrectAnswer: hc
  AnswerTests: omnitest(correctExpr='hc')
  Hint: Type hc at the command prompt.

- Class: mult_question
  Output: Which type of linkage did hclust() use to agglomerate clusters?
  AnswerChoices:  average; complete
  CorrectAnswer:  complete
  AnswerTests: omnitest(correctVal='complete')
  Hint: Look at the output when you looked at hc. What was the cluster method?





- Class: text
  Output: We see that range returned the minimum and maximum prices, so the diamonds vary in price from $326 to $18823. We've done the arithmetic for you, the range (difference between these two numbers) is $18497.

- Class: cmd_question
  Output:  Rerun qplot now with 3 arguments. The first is price, the second is data set equal to diamonds, and the third is binwidth set equal to 18497/30). (Use the up arrow to save yourself some typing.) See if the plot looks familiar.
  CorrectAnswer: qplot(price,data=diamonds,binwidth=18497/30)
  AnswerTests: omnitest(correctExpr='qplot(price,data=diamonds,binwidth=18497/30)')
  Hint: Type qplot(price,data=diamonds,binwidth=18497/30) at the command prompt.

- Class: text
  Output: No more messages in red, but a histogram almost identical to the previous one! If you typed 18497/30 at the command line you would get the result 616.5667. This means that the height of each bin tells you how many diamonds have a price between x and x+617 where x is the left edge of the bin. 

- Class: cmd_question
  Output:  We've created a vector containing integers that are multiples of 617 for you. It's call brk. Look at it now.
  CorrectAnswer: brk
  AnswerTests: omnitest(correctExpr='brk')
  Hint: Type brk at the command prompt.

- Class: cmd_question
  Output:  We've also created a vector containing the number of diamonds with  prices between each pair of adjacent entries of brk. For instance, the first count is the number of diamonds with prices between 0 and $617, and the second is the number of diamonds with prices between $617 and $1234. Look at this vector counts now. 
  CorrectAnswer: counts
  AnswerTests: omnitest(correctExpr='counts')
  Hint: Type counts at the command prompt.

- Class: text
  Output: See how it matches the histogram you just plotted? So, qplot really works!

- Class: cmd_question
  Output:  You're probably sick of it  but rerun qplot again, this time with 4 arguments. The first 3 are the same as the last qplot command you just ran  (price, data set equal to diamonds, and binwidth set equal to 18497/30). (Use the up arrow to save yourself some typing.) The fourth argument is fill set equal to cut. The shape of the histogram will be familiar, but it will be more colorful.
  CorrectAnswer: qplot(price,data=diamonds,binwidth=18497/30,fill=cut)
  AnswerTests: omnitest(correctExpr='qplot(price,data=diamonds,binwidth=18497/30,fill=cut)')
  Hint: Type qplot(price,data=diamonds,binwidth=18497/30,fill=cut) at the command prompt.

- Class: text
  Output: This shows how the counts within each price grouping (bin) are distributed among the different cuts of diamonds. Notice how qplot displays these distributions relative to the cut legend on the right. The fair cut diamonds are at the bottom of each bin, the good cuts are above them, then the very good above them, until the ideal cuts are at the top of each bin. You can quickly see from this display that there are very few fair cut diamonds priced above $5000.

- Class: cmd_question
  Output:  Now we'll replot the histogram as a density function which will show the proportion of diamonds in each bin. This means that the shape will be similar but the scale on the y-axis will be different since, by definition, the density function is nonnegative everywhere, and the area under the curve is one. To do this, simply call qplot with 3 arguments. The first 2 are  price and data (set equal to diamonds). The third is geom which should be set equal to the string "density". Try this now. 
  CorrectAnswer: qplot(price,data=diamonds,geom="density")
  AnswerTests: omnitest(correctExpr='qplot(price,data=diamonds,geom="density")')
  Hint: Type qplot(price,data=diamonds,geom="density") at the command prompt.

- Class: text
  Output:  Notice that the shape is similar to that of the histogram we saw previously. The highest peak is close to 0 on the x-axis meaning that most of the diamonds in the dataset were inexpensive. In general, as prices increase (move right along the x-axis) the number of diamonds (at those prices) decrease. The exception to this is when the price is around $4000; there's a slight increase in frequency. Let's see if cut is responsible for this increase. 

- Class: cmd_question
  Output: Rerun qplot, this time with 4 arguments. The first 2 are the usual, and the third is geom set equal to "density". The fourth is color set equal to cut. Try this now.
  CorrectAnswer: qplot(price,data=diamonds,geom="density",color=cut)
  AnswerTests: omnitest(correctExpr='qplot(price,data=diamonds,geom="density",color=cut)')
  Hint: Type qplot(price,data=diamonds,geom="density",color=cut) at the command prompt.

- Class: text
  Output: See how easily qplot did this? Four of the five cuts have 2 peaks, one at price $1000 and the other between $4000 and $5000. The exception is the Fair cut which has a single peak at $2500. This gives us a little more understanding of the histogram we saw before.

- Class: text
  Output: Let's move on to scatterplots. For these we'll need to specify two variables from the diamond dataset.

- Class: cmd_question
  Output:  Let's start with carat and price. Use these as the first 2 arguments of qplot. The third should be data set equal to the dataset. Try this now.
  CorrectAnswer: qplot(carat,price,data=diamonds)
  AnswerTests: omnitest(correctExpr=' qplot(carat,price,data=diamonds)')
  Hint: Type  qplot(carat,price,data=diamonds) at the command prompt.

- Class: text
  Output: We see the positive trend here, as the number of carats increases the price also goes up.

- Class: cmd_question
  Output:  Now rerun the same command, except add a fourth parameter, shape, set equal to cut.
  CorrectAnswer: qplot(carat,price,data=diamonds, shape=cut)
  AnswerTests: omnitest(correctExpr='qplot(carat,price,data=diamonds, shape=cut)')
  Hint: Type qplot(carat,price,data=diamonds, shape=cut) at the command prompt.

- Class: cmd_question
  Output:  The same scatterplot appears, except the cuts of the diamonds are distinguished by different symbols. The legend at the right tells you which symbol is associated with each cut. These are small and hard to read, so rerun the same command, except this time instead of setting the argument shape equal to cut, set the argument color equal to cut.
  CorrectAnswer: qplot(carat,price,data=diamonds, color=cut)
  AnswerTests: omnitest(correctExpr='qplot(carat,price,data=diamonds, color=cut)')
  Hint: Type qplot(carat,price,data=diamonds, color=cut) at the command prompt.

- Class: text
  Output: That's easier to see! Now we'll close with two, more complicated scatterplot examples.

- Class: cmd_question
  Output:  We'll rerun the plot you just did (carat,price,data=diamonds and color=cut) but add two more parameters. The first is the argument geom set equal to the concatenation of the 2 strings, "point" and "smooth". The second is the argument method set equal to the string "lm". Try this now.
  CorrectAnswer: qplot(carat,price,data=diamonds, color=cut,geom=c("point","smooth"),method="lm")
  AnswerTests: omnitest(correctExpr='qplot(carat,price,data=diamonds, color=cut,geom=c("point","smooth"),method="lm")')
  Hint: Type qplot(carat,price,data=diamonds, color=cut,geom=c("point","smooth"),method="lm") at the command prompt.


- Class: text
  Output: Again, we see the same scatterplot, but slightly more compressed and showing 5 regression lines, one for each cut of diamonds. It might be hard to see, but around each line is a shadow showing the 95% confidence interval. We see, unsurprisingly, that the better the cut, the steeper (more positive) the slope of the lines.

- Class: cmd_question
  Output:  Finally, let's rerun that plot you just did (carat,price,data=diamonds, color=cut, geom =c("point","smooth"),method="lm") but add one (just one) more argument. The new argument is facets and it should be set equal to the formula .~cut. Recall that the facets argument indicates we want a multi-panel plot. The symbol to the left of the tilde indicates rows (in this case just one) and the symbol to the right of the tilde indicates columns (in this five, the number of cuts). Try this now.
  CorrectAnswer: qplot(carat,price,data=diamonds, color=cut,geom=c("point","smooth"),method="lm",facets=.~cut)
  AnswerTests: omnitest(correctExpr='qplot(carat,price,data=diamonds, color=cut,geom=c("point","smooth"),method="lm",facets=.~cut)')
  Hint: Type qplot(carat,price,data=diamonds, color=cut,geom=c("point","smooth"),method="lm",facets=.~cut) at the command prompt.

- Class: text
  Output:  Pretty good, right? Not too difficult either. Let's review what we learned!


- Class: mult_question
  Output:  Which types of plot does qplot plot?
  AnswerChoices:  histograms; scatterplots; box and whisker plots; all of the others
  CorrectAnswer: all of the others
  AnswerTests: omnitest(correctVal='all of the others')
  Hint: That qplot is amazing! It seems to do everything!

- Class: mult_question
  Output:  Any and all of the above choices work; qplot is just that good. What does the gg in ggplot2 stand for?
  AnswerChoices:  good grief; grammar of graphics; goto graphics; good graphics
  CorrectAnswer: grammar of graphics
  AnswerTests: omnitest(correctVal='grammar of graphics')
  Hint: Think of building blocks and components, also nouns, verbs, and diagramming sentences.

- Class: mult_question
  Output:  True or False? The geom argument takes a string for a value.
  AnswerChoices:  True; False
  CorrectAnswer: True
  AnswerTests: omnitest(correctVal='True')
  Hint: Recall our examples, for instance, geom="density".

- Class: mult_question
  Output:  True or False? The method argument takes a string for a value.
  AnswerChoices:  True; False
  CorrectAnswer: True
  AnswerTests: omnitest(correctVal='True')
  Hint: Recall our examples, for instance, method="lm".

- Class: mult_question
  Output:  True or False? The binwidth argument takes a string for a value.
  AnswerChoices:  True; False
  CorrectAnswer: False
  AnswerTests: omnitest(correctVal='False')
  Hint: Recall our examples, for instance, binwidth=18497/30.

- Class: mult_question
  Output:  True or False? The user must specify x- and y-axis labels when using qplot. 
  AnswerChoices:  True; False
  CorrectAnswer: False
  AnswerTests: omnitest(correctVal='False')
  Hint: Recall our examples when we saw labels that we didn't specify.

- Class: text
  Output: Now for some ggplots.

- Class: cmd_question
  Output:  First create a graphical object g by assigning to it the output of a call to the function ggplot with 2 arguments. The first is the dataset diamonds and the second is a call to the function aes with 2 arguments, depth and price. Remember you won't see any result.
  CorrectAnswer: g <- ggplot(diamonds,aes(depth,price))
  AnswerTests: omnitest(correctExpr='g <- ggplot(diamonds,aes(depth,price))')
  Hint: Type g <- ggplot(diamonds,aes(depth,price)) at the command prompt.

- Class: cmd_question
  Output:  Does g exist? Yes! Type summary with g as an argument to see what it holds.
  CorrectAnswer: summary(g)
  AnswerTests: omnitest(correctExpr='summary(g)')
  Hint: Type summary(g) at the command prompt.

- Class: cmd_question
  Output:  We see that g holds the entire dataset. Now suppose we want to see a scatterplot of the relationship. Add to g a call to the function geom_point with 1 argument, alpha set equal to 1/3.
  CorrectAnswer: g+geom_point(alpha=1/3)
  AnswerTests: omnitest(correctExpr='g+geom_point(alpha=1/3)')
  Hint: Type g+geom_point(alpha=1/3) at the command prompt.

- Class: text
  Output:  That's somewhat interesting. We see that depth ranges from 43 to 79, but the densest distribution is around 60 to 65. Suppose we want to see if this relationship (between depth and price) is affected by cut or carat. We know cut is a factor with 5 levels (Fair, Good, Very Good, Premium, and Ideal). But carat is  numeric and not a discrete factor. Can we do this?

- Class: text
  Output: Of course! That's why we asked. R has a handy command, cut, which allows you to divide your data into sets and label each entry as belonging to one of the sets, in effect creating a new factor. First, we'll have to decide where to cut the data. 

- Class: cmd_question
  Output: Let's divide the data into 3 pockets, so 1/3 of the data falls into each. We'll use the R command quantile to do this. Create the variable cutpoints and assign to it the output of a call to the function quantile with 3 arguments. The first is the data to cut, namely diamonds$carat; the second is a call to the R function seq. This is also called with 3 arguments, (0, 1, and length set equal to 4). The third argument to the call to quantile is the boolean na.rm set equal to TRUE.
  CorrectAnswer: cutpoints <- quantile(diamonds$carat,seq(0,1,length=4),na.rm=TRUE)
  AnswerTests: expr_creates_var("cutpoints"); omnitest(correctExpr='cutpoints <- quantile(diamonds$carat,seq(0,1,length=4),na.rm=TRUE)')
  Hint: Type cutpoints <- quantile(diamonds$carat,seq(0,1,length=4),na.rm=TRUE) at the command prompt.

- Class: cmd_question
  Output: Look at cutpoints now to understand what it is.
  CorrectAnswer: cutpoints 
  AnswerTests:  omnitest(correctExpr='cutpoints')
  Hint: Type cutpoints  at the command prompt.

- Class: cmd_question
  Output: We see a 4-long vector (explaining why length was set equal to 4). We also see that .2 is the smallest carat size in the dataset and 5.01 is the largest. One third of the diamonds are between .2 and .5 carats and another third are between .5 and 1 carat in size. The remaining third are between 1 and 5.01 carats. Now we can use the R command cut to label each of the 53940 diamonds in the dataset as belonging to one of these 3 factors. Create a new name in diamonds, diamonds$car2 by assigning it the output of the call to cut. This command takes 2 arguments, diamonds$carat, which is what we want to cut, and cutpoints, the places where we'll cut.
  CorrectAnswer: diamonds$car2 <- cut(diamonds$carat,cutpoints)
  AnswerTests:  omnitest(correctExpr='diamonds$car2 <- cut(diamonds$carat,cutpoints)')
  Hint: Type diamonds$car2 <- cut(diamonds$carat,cutpoints)  at the command prompt.

- Class: cmd_question
  Output: Now we can continue with our multi-facet plot. Recall the last plotting expression you typed was g+geom_point(alpha=1/3). Recall this expression (or retype it) and add a call to the function facet_grid using the formula cut ~ car2 as its argument.
  CorrectAnswer: g+geom_point(alpha=1/3)+facet_grid(cut~car2)
  AnswerTests:  omnitest(correctExpr='g+geom_point(alpha=1/3)+facet_grid(cut~car2)')
  Hint: Type g+geom_point(alpha=1/3)+facet_grid(cut~car2)  at the command prompt.

- Class: text
  Output: We see a multi-facet plot with 5 rows, each corresponding to a cut factor. Not surprising. What is surprising is the number of columns. We were expecting 3 and got 4. Why?

- Class: cmd_question
  Output: The first 3 columns are labeled with the cutpoint boundaries. The fourth is labeled NA and shows us where the data points with missing data (NA or Not Available) occurred. We see that there were only a handful (12 in fact) and they occurred in Very Good, Premium, and Ideal cuts. We created a vector, myd,  containing the indices of these datapoints. Look at these entries in diamonds by typing the expression diamonds[myd,]. The myd tells R what rows to show and the empty column entry says to print all the columns. 
  CorrectAnswer: diamonds[myd,]
  AnswerTests:  omnitest(correctExpr='diamonds[myd,]')
  Hint: Type diamonds[myd,]  at the command prompt.

- Class: text
  Output: We see these entries match the plots. Whew - that's a relief. The car2 field is, in fact, NA for these entries, but the carat field shows they each had a carat size of .2. What's going on here?

- Class: text
  Output: Actually our plot answers this question. The boundaries for each column appear in the gray labels at the top of each column, and we see that the first column is labeled (0.2,0.5]. This indicates that this column contains data greater than .2 and less than or equal to .5. So diamonds with carat size .2 were excluded from the car2 field. 

- Class: cmd_question
  Output: Finally, recall the last plotting command (g+geom_point(alpha=1/3)+facet_grid(cut~car2)) or retype it if you like and add another call. This one to the function geom_smooth. Pass it 3 arguments, method set equal to the string "lm", size set equal to 3, and color equal to the string "pink".
  CorrectAnswer: g+geom_point(alpha=1/3)+facet_grid(cut~car2)+geom_smooth(method="lm",size=3,color="pink")
  AnswerTests:  omnitest(correctExpr='g+geom_point(alpha=1/3)+facet_grid(cut~car2)+geom_smooth(method="lm",size=3,color="pink")')
  Hint: Type g+geom_point(alpha=1/3)+facet_grid(cut~car2)+geom_smooth(method="lm",size=3,color="pink")  at the command prompt.

- Class: text
  Output: Nice thick regression lines which are somewhat interesting. You can add labels to the plot if you want but we'll let you experiment on your own.

- Class: cmd_question
  Output: Lastly, ggplot2 can, of course, produce boxplots. This final exercise is the sum of 3 function calls. The first call is to ggplot with 2 arguments, diamonds and a call to aes with carat and price as arguments. The second call is to geom_boxplot with no arguments. The third is to facet_grid with one argument, the formula . ~ cut. Try this now.
  CorrectAnswer: ggplot(diamonds,aes(carat,price))+geom_boxplot()+facet_grid(.~cut)
  AnswerTests:  omnitest(correctExpr='ggplot(diamonds,aes(carat,price))+geom_boxplot()+facet_grid(.~cut)')
  Hint: Type ggplot(diamonds,aes(carat,price))+geom_boxplot()+facet_grid(.~cut) at the command prompt.

- Class: text
  Output: Yes! A boxplot looking like marshmallows about to be roasted. Well done and  congratulations! You've finished this jewel of a lesson. Hope it payed off!

