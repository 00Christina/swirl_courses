- Class: meta
  Course: Statistical_Inference
  Lesson: T_Confidence_Intervals
  Author: Swirl Coders
  Type: Coursera
  Organization: Johns Hopkins Bloomberg School of Public Health
  Version: 2.2.0

- Class: text
  Output: "T_Confidence_Intervals. (Slides for this and other Data Science courses may be found at github https://github.com/DataScienceSpecialization/courses/. If you care to use them, they must be downloaded as a zip file and viewed locally. This lesson corresponds to 06_Statistical_Inference/08_tCIs.)"

- Class: text
  Output: In this lesson, we'll discuss some statistical methods for dealing with small datasets, specifically the Student's or Gosset's t distribution and t confidence intervals. 

- Class: mult_question
  Output: In the Asymptotics lesson we discussed confidence intervals using the Central Limit Theorem (CLT) and normal distributions. These needed large sample sizes, and the formula for computing the interval was E +/- qnorm *std error(E), where E was some estimated value such as a sample mean with a standard error. Here qnorm represented what?
  AnswerChoices: the population mean; the population variance; the standard error; a specified quantile from a normal distribution 
  CorrectAnswer: a specified quantile from a normal distribution
  AnswerTests: omnitest(correctVal='a specified quantile from a normal distribution')
  Hint: Which choice has part of the word 'qnorm' in it?

- Class: text
  Output: For t distributions, the formula for computing a confidence interval is similar. However, instead of a quantile for a normal distribution we use a quantile for a t distribution. So the formula is  E +/- t-quantile *std error(E).

- Class: text
  Output: These t confidence intervals are very handy, and if you have a choice between these and normal, pick these. We'll see that as datasets get larger, t-intervals look normal. We'll cover the one and two group versions.

- Class: text
  Output: The t distribution, invented by William Gosset in 1908, has thicker tails than the normal. Also, instead of having two parameters, mean and variance, as the normal does, the t distribution has only one - the number of degrees of freedom (df). 

- Class: text
  Output: As df increases, the t distribution gets more like a standard normal, so it's centered around 0. Also, the t assumes that the underlying data are iid Gaussian so (X' - mu}/(S/sqrt(n)) follows the t distribution with n-1 degrees of freedom. Notice another difference from the normal confidence intervals? Here, for the standard error, we use the sample standard deviation divided by the sqrt of the sample size. That's nice, right? 

- Class: mult_question
  Output: In the formula Z=(X' - mu}/(S/sqrt(n)), if we replaced S by sigma the statistic Z would be what?.
  AnswerChoices: the standard normal; the standard abnormal; the population variance; Huh?
  CorrectAnswer: the standard normal
  AnswerTests: omnitest(correctVal='the standard normal')
  Hint: With the replacement the formula should look familiar, like a standardized normal perhaps?

- Class: figure
  Output: To see what we mean, we've taken code from the slides, the function myplot, which takes the integer df as its input and plots the t distribution with df degrees of freedom. It also plots a standard normal distribution so you can see how they relate to one another. 
  Figure: tPlot.R
  FigureType: new

- Class: cmd_question
  Output: Try myplot now with an input of 2.
  CorrectAnswer: myplot(2)
  AnswerTests: omnitest(correctExpr='myplot(2)')
  Hint: Type myplot(2) at the command prompt.

- Class: cmd_question
  Output: You can see that the hump of t distribution (in blue) is not as high as the normal's and the two tails of the t distribution absorb the extra mass, so they're thicker. Note that with 2 degrees of freedom, you only have 3 data points. Ha! Talk about small sample sizes. Now try myplot  with an input of 20.
  CorrectAnswer: myplot(20)
  AnswerTests: omnitest(correctExpr='myplot(20)')
  Hint: Type myplot(20) at the command prompt.

- Class: figure
  Output: The two distributions are almost right on top of each other using this higher degree of freedom. Another way to look at these distributions is to plot their quantiles. From the slides, we've provided a second function for you, myplot2, which does this. It plots a lightblue reference line representing normal quantiles and a black line for the t quantiles. Both plot the quantiles from .5 which start at 0 (since the distributions are symmetric about 0) to .99.
  Figure: tQuant.R
  FigureType: new

- Class: cmd_question
  Output: Try myplot2 now with an argument of 2.
  CorrectAnswer: myplot2(2)
  AnswerTests: omnitest(correctExpr='myplot2(2)')
  Hint: Type myplot2(2) at the command prompt.

- Class: text
  Output: The distance between the two lines represents the difference in sizes between the two sets of intervals. Note the thin horizontal and vertical lines. These represent the .975 quantiles for the t and normal distributions respectively. You probably recognized the placement of the vertical at 1.96 from the Asymptotics lesson. 

- Class: cmd_question
  Output: Check the placement of the horizontal now using the R function qt with the arguments .975 for the quantile and 2 for the degrees of freedom (df).
  CorrectAnswer: qt(.975,2)
  AnswerTests: omnitest(correctExpr='qt(.975,2)')
  Hint: Type qt(.975,2) at the command prompt.

- Class: cmd_question
  Output: Now run myplot2 with an argument of 20.
  CorrectAnswer: myplot2(20)
  AnswerTests: omnitest(correctExpr='myplot2(20)')
  Hint: Type myplot2(20) at the command prompt.

- Class: text
  Output: The quantiles are much closer together with the higher degrees of freedom. At the 97.5 percentile though the t quantile is still greater than the normal. Student's Rule!

- Class: text
  Output: As we said before, the t-interval is X' +/- t_(n-1)*s/sqrt(n) where t_(n-1) is the relevant quantile. The t interval  assumes that the data are iid normal, though it is robust to this assumption and  works well whenever the distribution of the data is roughly symmetric and mound shaped.

- Class: mult_question
  Output: Our plots showed us that for large degrees of freedom, t quantiles become the same as what?
  AnswerChoices: standard normal quantiles; standard abnormal quantiles; very large numbers; very small numbers
  CorrectAnswer: standard normal quantiles
  AnswerTests: omnitest(correctVal='standard normal quantiles')
  Hint: Recall that the larger the degrees of freedom, the more the t distribution looked normal. Smaller degrees of freedom made it look abnormal. 

- Class: text
  Output: Although it's pretty great, the t interval isn't always applicable. For skewed distributions, the spirit of the t interval assumptions are violated and it doesn't make a lot of sense to center the interval at the mean. There are ways of working around this problem (such as taking logs or using a different summary like the median).
     
- Class: text
  Output: For highly discrete data, like binary, intervals other than the t are available. 

- Class: text
  Output: However, paired observations are often analyzed using the t interval by taking differences between the observations. We'll show you what we mean now.

- Class: text
  Output: We hope you're not tired because we're going to look at some sleep data. This was the data originally analyzed in Gosset's Biometrika paper, which shows the increase in hours for 10 patients on two soporific drugs. 

- Class: cmd_question
  Output: We've loaded the data for you. R treats it as two groups rather than paired. To see what we mean type sleep now. This will show you how the data is stored. 
  CorrectAnswer: sleep
  AnswerTests: omnitest(correctExpr='sleep')
  Hint: Type sleep at the command prompt.

- Class: figure
  Output: We see 20 entries, the first 10 show the results (extra) of the first drug (group 1) on each of the patients (ID), and the last 10 entries the results of the second drug (group 2) on each patient (ID). Here we've plotted the data, connecting each patient's two results with a line, group 1 on the left and group 2 on the right. See that purple line with the steep slope? That's ID 9, with 0 result for group 1 and 4.6 for group 2. 
  Figure: sleepPlot.R
  FigureType: new

- Class: text
  Output: If  we just looked at the 20 data points we'd be comparing group 1 variations with group 2 variations. However, when we look at the data paired for each patient, we see that the variations in results  are subject dependent. 

- Class: cmd_question
  Output: To clarify, we've defined some variables for you, namely g1 and g2. These are two 10-long vectors, respectively holding the results of the 10 patients for each of the two drugs. Look at the range of g1 using the R command range. 
  CorrectAnswer: range(g1)
  AnswerTests: omnitest(correctExpr='range(g1)')
  Hint: Type range(g1) at the command prompt.

- Class: cmd_question
  Output: Now look at the range of g2. We see that the ranges of both groups are relatively large.
  CorrectAnswer: range(g2)
  AnswerTests: omnitest(correctExpr='range(g2)')
  Hint: Type  range(g2) at the command prompt.

- Class: cmd_question
  Output:  Now let's look at the pairwise difference. We can take advantage of R's componentwise subtraction of vectors and create the vector difference by subtracting g1 from g2. Do this now.
  CorrectAnswer: difference <- g2-g1
  AnswerTests: expr_creates_var("difference"); omnitest(correctExpr='difference <- g2-g1')
  Hint: Type  difference <- g2-g1 at the command prompt.

- Class: cmd_question
  Output:  Now use the R function mean to find the average of the vector difference and put the result in the variable mn.
  CorrectAnswer: mn <- mean(difference)
  AnswerTests: expr_creates_var("mn"); omnitest(correctExpr='mn <- mean(difference)')
  Hint: Type mn <- mean(difference) at the command prompt.

- Class: text
  Output: See how much smaller the mean difference in this paired data compared to the group variations?

- Class: cmd_question
  Output:  Now use the R function sd to find the standard deviation of the vector difference and put the result in the variable s.
  CorrectAnswer: s <- sd(difference)
  AnswerTests: expr_creates_var("s"); omnitest(correctExpr='s <- sd(difference)')
  Hint: Type s <- sd(difference) at the command prompt.

- Class: cmd_question
  Output:  Now recall the formula for finding the t confidence interval, X' +/- t_(n-1)*s/sqrt(n). Make the appropriate substitutions to find the 95% confidence intervals for this data. Remember to use the R construct c(-1,1) for the +/- portion of the formula and the R function qt with .975 and n-1 degrees of freedom for the quantile portion. Our data size is 10.
  CorrectAnswer: mn + c(-1,1)*qt(.975,9)*s/sqrt(10)
  AnswerTests:  omnitest(correctExpr='mn + c(-1,1)*qt(.975,9)*s/sqrt(10)')
  Hint: Type mn + c(-1,1)*qt(.975,9)*s/sqrt(10) at the command prompt.

- Class: cmd_question
  Output:  We could also just have used the R function t.test with the argument difference to get this result. As with the other R test functions, this returns a lot of information. Since all we're interested in at the moment is the confidence interval we can pick this off with the construct x$conf.int. Try this now.
  CorrectAnswer: t.test(difference)$conf.int
  AnswerTests:  omnitest(correctExpr='t.test(difference)$conf.int')
  Hint: Type t.test(difference)$conf.int at the command prompt.

- Class: video
  Output: As the slides showed,  R provides several ways of using t.test to find the confidence interval of this data. Would you like to see the R code of these alternatives and how to display them nicely? You'll need an internet connection to see it.
  VideoLink: "http://wilcrofter.github.io/slidex/markDown/ttest.html"

- Class: text
  Output: We now present methods, using t confidence intervals, for comparing independent groups.
Suppose that we want to compare the mean blood pressure between two groups in a randomized trial; those who received the treatment to those who received a placebo. Unlike the sleep study, we cannot use the paired t test because the groups are independent and may have different sample sizes.

- Class: text
  Output: So our goal is to find a 95% confidence interval of the diffence between two population means. Let's represent this difference as mu_y - mu_x. How do we do this? Recall our formula X' +/- t_(n-1)*s/sqrt(n)

- Class: text
  Output: First we need a sample mean, but we have two, X' and Y', one from each group. It makes sense that we'd have to take their difference (Y'-X') as well, since we're looking for a confidence interval that contains the difference mu_y-mu_x. Now we need to specify a t quantile. Suppose the groups have different sizes n_x and n_y. 

- Class: mult_question
  Output: For one group we used the  quantile t_(.975,n-1). What do you think we'll use for the quantile of this problem?
  AnswerChoices: t_(.975,n_x-1); t_(.975,n_y-1); t_(.975,n_x+n_y-1); t_(.975,n_x+n_y-2)
  CorrectAnswer: t_(.975,n_x+n_y-2)
  AnswerTests: omnitest(correctVal='t_(.975,n_x+n_y-2)')
  Hint: We lose one degree of freedom from each group because we've calculated the sample mean from each group, so we add the two sizes and subtract two. 

- Class: text
  Output: The only term remaining is the standard error which for the single group is S/sqrt(n). Our interval will assume for now a common variance S^2 across the two groups. We'll actually pool variance information from the two groups. (We'll deal with the more complicated situation later.) How do we handle the 1/sqrt(n) portion? We can simply add 1/n_x and 1/n_y and take the square root of the sum.

- Class: text
  Output: We call the variance estimator we use the pooled variance S^2. The formula for it requires two variance estimators, one for each group. We multiply each by its respective degrees of freedom and divide the sum by the total number of degrees of freedom.

- Class: mult_question
  Output: Which of the following represents the numerator of this expression?
  AnswerChoices: (n_x-1)(S_x)^2+(n_y-1)(S_y)^2; (n_x)(S_x)^2+(n_y)(S_y)^2; (n_x)(S_x)+(n_y)(S_y) 
  CorrectAnswer: (n_x-1)(S_x)^2+(n_y-1)(S_y)^2
  AnswerTests: omnitest(correctVal='(n_x-1)(S_x)^2+(n_y-1)(S_y)^2')
  Hint: We need variances so the choice without the squared S terms is incorrect. Recall that the degrees of freedom is one less than the sample size for each group so that eliminates another choice and only one choice remains.

- Class: mult_question
  Output: Which of the following represents the total number of degrees of freedom?
  AnswerChoices: (n_x-1)+(n_y-1); (n_x+n_y); (n_x+n_y-1); (n_x+n_y+2)
  CorrectAnswer: (n_x-1)+(n_y-1)
  AnswerTests: omnitest(correctVal='(n_x-1)+(n_y-1)')
  Hint: Recall that the degrees of freedom is one less than the sample size for each group. We asked this a few questions ago, though we've put this answer in a different, but equivalent form.

- Class: text
  Output: The Law of Large Numbers (LLN) says that the average (mean) approaches what it's estimating. We saw in our simulations that the larger the sample size the better the estimation.  As we flip a fair coin over and over, it eventually converges to the true probability of a head (.5). 

- Class: text
  Output: The LLN forms the basis of frequency style thinking. 

- Class: cmd_question
  Output: To see this in action, we've copied some code from the slides and created the function coinPlot. It takes an integer n which is the number of coin tosses that will be simulated. As coinPlot does these coin flips it computes the cumulative sum (assuming heads are 1 and tails 0), but after each toss it divides the cumulative sum by the number of flips performed so far. It then plots this value for each of the k=1...n tosses. Try it now for n=10. 
  CorrectAnswer: coinPlot(10)
  AnswerTests: omnitest(correctExpr='coinPlot(10)')
  Hint: Type coinPlot(10) at the command prompt.

- Class: cmd_question
  Output: Your output depends on R's random number generator, but your plot probably jumps around a bit and, by the 10th flip, your cumulative sum/10 is probably different from mine. If you did this several times, your plots would vary quite a bit. Now call coinPlot again, this time with 10000 as the argument.
  CorrectAnswer: coinPlot(10000)
  AnswerTests: omnitest(correctExpr='coinPlot(10000)')
  Hint: Type coinPlot(10000) at the command prompt.

- Class: text
  Output: See the difference? Asymptotics in Action! The line approaches its asymptote of .5. This is the probability you expect since what we're plotting, the cumulative sum/number of flips, represents the probability of the coin landing on heads. As we know,  this is .5 .

- Class: text
  Output: We say that an estimator is CONSISTENT if it converges to what it's trying to estimate. The LLN says that the sample mean of iid samples is consistent for the population mean. This is good, right?

- Class: mult_question
  Output: Based on our previous lesson do you think the sample variance (and hence sample deviation) are consistent as well?
  AnswerChoices: Yes; No
  CorrectAnswer: Yes
  AnswerTests: omnitest(correctVal='Yes')
  Hint: Recall our simulations of sample variances and how, as we increased the sample size, they converged to the population variance. Sounds like consistency, right?

- Class: text
  Output: Now for something really important - the CENTRAL LIMIT THEOREM (CLT) - one of the most important in all of statistics. It states that the distribution of averages of iid variables (properly normalized) becomes that of a standard normal as the sample size increases. 
 
- Class: text
  Output: Let's dissect this to see what it means. First, 'properly normalized' means that you transformed the sample mean X'. You subtracted the population mean mu from it and divided the difference by sigma/sqrt(n). Here sigma is the standard deviation of the population and n is the sample size. 

- Class: text
  Output: Second, this normalized variable, (X'-mu)/(sigma/sqrt(n)) is a random variable with a normal distribution with mean 0 and variance 1. Remember that n must be large for the CLT to apply.

- Class: mult_question
  Output: Do you recognize sigma/sqrt(n) from our lesson on variance? Since the population std deviation sigma is unknown, sigma/sqrt(n) is often approximated by what? 
  AnswerChoices: the standard error of the sample mean; the variance of the population; the mean of the population; I give up
  CorrectAnswer: the standard error of the sample mean
  AnswerTests: omnitest(correctVal='the standard error of the sample mean')
  Hint: Recall our many simulation experiments in the variance lesson where we calculated standard deviations of means using R's sd function, then we calculated an approximation using a formula involving the population variance and the square root of the sample size.

 Class: mult_question
  Output: Suppose you found the sample mean of a set of size 3. Could you use the CLT to say that the sample mean was normally distributed?
  AnswerChoices: Yes!; No, the sample isn't big enough
  CorrectAnswer: No, the sample isn't big enough
  AnswerTests: omnitest(correctVal='No, the sample isn't big enough')
  Hint: Remember, the CLT says that as the sample size needs to be large and 3 really isn't large.

- Class: text
  Output: Let's rephrase the CLT. Suppose X_1, X_2, ... X_n are independent random variables from an infinite population with mean mu and variance sigma^2. Then if n is large, the mean of the X's, call it X', is approximately normal with mean mu and variance sigma^2/n. We denote this as Z'~N(mu,sigma^2/n).

- Class: figure
  Output: To show the CLT in action consider this figure from the slides. It presents 3 histograms of 1000 averages of dice rolls with sample sizes of 10, 20 and 30 respectively. Each average of n dice rolls (n=10,20,30) has been normalized by subtracting off the mean (3.5) then dividing by the standard error, sqrt(2.92/n). The normalization has made each histogram look like a standard normal, i.e., mean 0 and standard deviation 1.
  Figure: cltDice.R
  FigureType: new

- Class: text
  Output: Notice that the CLT said nothing about the original population being normally distributed. That's precisely where its usefulness lies! We can assume normality of a population mean no matter what kind of population we have, as long as our sample size is large enough. Let's look at how it works with a binomial experiment like flipping a coin.

- Class: text
  Output: Recall that if the probability of a head (call it 1) is p, then the probability of a tail (0) is 1-p. The expected value then is p and the variance is p-p^2 or p(1-p). Suppose we do n coin flips and let p' represent the average of these n flips. We normalize p' by subtracting p and dividing by sqrt(p(1-p)/n). Let's do this for 1000 trials and plot the resulting histogram. 

- Class: figure
  Output: Here's a figure from the slides showing the results of 3 such trials where each trial is for a different value of n (10, 20, and 30) and the coin is fair,so E(X)=.5 and the standard error is 1/(2sqrt(n)). Notice how with larger n (30) the histogram tightens up around the mean 0.
  Figure: cltFairCoin.R
  FigureType: new

- Class: figure
  Output: Here's another plot from the slides of the same experiment, this time using a biassed coin. We set the probability of a head to .9, so E(X)=.9 and the standard error is sqrt(.09/n) Again, the larger the sample size the more closely the distribution looks normal, although with this biassed coin the normal approximation isn't as good as it was with the fair coin.
  Figure: cltUnfairCoin.R
  FigureType: new

- Class: text
  Output: Now let's talk about confidence intervals.

- Class: figure
  Output:  We know from the CLT that for large n, the sample mean is normal with mean mu and standard deviation sigma/sqrt(n). We also know that 95% of the area under a normal curve is within two standard deviations of the mean. This figure, a standard normal with mu=0 and sigma=1, illustrates this point; the entire shaded portion depicts the area within 2 standard deviations of the mean and the darker portion shows the 68% of the area within 1 standard deviation. 
  Figure: stddev1.R
  FigureType: new

- Class: text
  Output: It follows that 5% of the area under the curve is not shaded. By symmetry of the curve, only 2.5% of the data is greater than the mean + 2 standard deviations (mu+2*sigma/sqrt(n)) and only 2.5% is less than the  mean - 2 standard deviations (mu-2*sigma/sqrt(n)). 

- Class: text
  Output: So the probability that the sample mean X' is bigger than mu + 2sigma/sqrt(n) OR smaller than mu-2sigma/sqrt(n) is 5%.  Equivalently, the probability of being between these limits is 95%. 

- Class: text
  Output: The quantity X' plus or minus 2 sigma/sqrt(n) is called a 95% interval for mu. The 95% says that if one were to repeatly get samples of size n, about 95% of the intervals obtained would contain mu, the quantity we're trying to estimate. 

- Class: mult_question
  Output: Note that for a 95% confidence interval we divide (100-95) by 2 (since we have both left and right tails) and add the result to 95 to compute the quantile we need. The 97.5 quantile is actually 1.96, but for simplicity it's often just rounded up to 2. If you wanted to find a 90% confidence interval what quantile would you want?
  AnswerChoices: 90; 95; 85; 100
  CorrectAnswer: 95
  AnswerTests: omnitest(correctVal='95')
  Hint: Divide (100-90) by 2 and add this result to 90.

- Class: cmd_question
  Output: Use the R function qnorm to find the 95 quantile for a standard normal distribution. Remember that qnorm takes a probability as an input. You can use default values for all the other arguments.
  CorrectAnswer: qnorm(.95)
  AnswerTests: omnitest(correctExpr='qnorm(.95)')
  Hint: Type qnorm(.95) at the command prompt.

- Class: mult_question
  Output: As we've seen before, in a binomial distribution in which p represents the probability or proportion of success, the variance sigma^2 is p(1-p), so the standard error of the sample mean p' is sqrt(p(1-p)/n) where n is the sample size. The 95% confidence interval of p is then p' +/- 2*sqrt(p(1-p)/n). The 2 in this formula represents what?
  AnswerChoices: the standard error of p'; the mean of p'; the variance of p'; the approximate 95 quantile
  CorrectAnswer: the approximate 95 quantile
  AnswerTests: omnitest(correctVal='the approximate 95 quantile')
  Hint: Divide (100-90) by 2 and add this result to 90.

(As before, we use 2 as the 97.5 quantile.) We don't know the true value of p; that's what we're trying to estimate. The Wald confidence interval replaces p with p'.

- Class: text
  Output:  Since we don't know p, we want to maximize the 95% confidence interval represented by 2*sqrt(p(1-p)/n). From calculus we know that p(1-p) is maximized when p=1/2, so our biggest 95% confidence interval estimate would use 1/2 for a value of p in the formula p'+/- 2*sqrt(p(1-p)/n). 

- Class: mult_question
  Output: Using 1/2 for the value of p in the formula above yields what 95% confidence interval for p?
  AnswerChoices: p'+/- 1/sqrt(n); p'+/- 1/2sqrt(n); p'+/ 2sqrt(n)
  CorrectAnswer: p'+/- 1/sqrt(n)
  AnswerTests: omnitest(correctVal='p\'+/- 1/sqrt(n)')
  Hint: p(1-p)=1/4 when p=1/2 and the sqrt(1/4n)=1/(2sqrt(n)). What happens when you multiply this by 2?

- Class: mult_question
  Output: We'll illustrate how to use this formula with an example from the slides. Suppose you were running for office and your pollster polled 100 people. Of these 60 claimed they were going to vote for you. You'd like to estimate the true proportion of people who will vote for you and you want to be 95% confident of your estimate. We need to find the limits that will contain the true proportion of your supporters with 95% confidence, so we'll use the formula p' +/- 1/sqrt(n) to answer this question. First, what value would you use for p', the sampled estimate?
  AnswerChoices: .60; .56; 1.00; .10
  CorrectAnswer: .60
  AnswerTests: omnitest(correctVal='.60')
  Hint: The only sampled number here is the number of people who said they would vote for you. Make it a proportion by dividing it by the sample size.

- Class: mult_question
  Output: What would you use for 1/sqrt(n)?
  AnswerChoices: 1/sqrt(60); 1/sqrt(56); 1/100; 1/10
  CorrectAnswer: 1/10
  AnswerTests: omnitest(correctVal='1/10')
  Hint: The sample size is n, and in this case n=100. What is 1/sqrt(100)?

- Class: mult_question
  Output: The bounds of the interval then are what?
  AnswerChoices: .5 and .7; .46 and .66; .55 and .65; I haven't a clue
  CorrectAnswer: .5 and .7
  AnswerTests: omnitest(correctVal='.5 and .7')
  Hint: We know p'- 1/sqrt(n) is the lower bound and p'+ 1/sqrt(n) is the upper bound, so use the answers from the two previous answers to fill in values for these variables.

- Class: mult_question
  Output: How do you feel about the election?
  AnswerChoices: confident; unsure; I'll pull out;  Perseverance, that's the answer; 
  CorrectAnswer: confident
  AnswerTests: omnitest(correctVal='confident')
  Hint: With 95% confidence, between .5 and .7 of the voters support you. You look like a winner to me!

- Class: cmd_question
  Output: To reassure you that this is a pretty good estimate, we can use R to get a more precise confidence interval using the actual quantile 1.96 instead of our ballpark estimate of 2. Use the formula  p'+/- qnorm(.975)*sqrt(p'(1-p')/100). Use the p' and n values from above and the R construct p'+c(-1,1)... to handle the plus/minus portion of the formula. You should see bounds similar to the ones you just estimated.
  CorrectAnswer: .6 + c(-1,1)*qnorm(.975)*sqrt(.6*.4/100)
  AnswerTests: omnitest(correctExpr='.6 + c(-1,1)*qnorm(.975)*sqrt(.6*.4/100)')
  Hint: Type .6 + c(-1,1)*qnorm(.975)*sqrt(.6*.4/100) at the command prompt. 

- Class: cmd_question
  Output: As an alternative to this Wald interval, we can also use the R function binom.test with the parameters 60 and 100 and let all the others default. This function "performs an exact test of a simple null hypothesis about the probability of success in a Bernoulli experiment." (This means it guarantees the coverages, uses a lot of computation and doesn't rely on the CLT.) This function returns a lot of information but all we want to look at now is the values of the confidence interval, conf.int, that it returns. Use the R construct x$conf.int to find these now.
  CorrectAnswer: binom.test(60,100)$conf.int
  AnswerTests: omnitest(correctExpr='binom.test(60,100)$conf.int')
  Hint: Type binom.test(60,100)$conf.int at the command prompt. 

- Class: text
  Output: Close to what we've seen before, right? Now we're going to see that the Wald interval isn't very accurate when n is small. We'll use the example from the slides. 

- Class: figure
  Output: Suppose we flip a coin a small number of times, say 20. Also suppose we have a function mywald which takes a probability p, and generates 30 sets of 20 coin flips using that probability p. It uses the sampled proportion of success, p', for those 20 coin flips to compute the upper and lower bounds of the 95% Wald interval, that is, it computes the two numbers p'+/- qnorm(.975) * sqrt(p' * (1-p') / n) for each of the 30 trials. For the given true probability p, we  count the number of times out of those 30 trials that the true probability p was in the Wald confidence interval. We'll call this the coverage.
  Figure: WaldDemo.R
  FigureType: new

- Class: cmd_question
  Output: To make sure you understand what's going on, trying running mywald now with the probability .2. It will print out 30 p' values (which you don't really need to see), followed by 30 lower bounds, 30 upper bounds and lastly the percentage of times that the input .2 was between the two bounds. See if you agree with the percentage you get. Usually it suffices to just count the number of times (out of the 30) that .2 is less than the upper bound.
  CorrectAnswer: mywald(.2)
  AnswerTests: omnitest(correctExpr='mywald(.2)')
  Hint: Type mywald(.2) at the command prompt. 

- Class: text
  Output: Now that you understand the underlying principle, suppose instead of 30 trials, we used 1000 trials. Also suppose we did this experiment for a series of probabilities, say from .1 to .9 taking steps of size .05. More specifically, we'll call our function using 17 different probabilities, namely .1, .15, .2, .25, ... .9 . We can then plot the percentages of coverage for each of the probabilities. 

- Class: figure
  Output: Here's the plot of our results. Each of the 17 vertices show the percentage of coverage for a particular true probability p passed to the function. Results will vary, but usually the only probability that hits above the 95% line is the p=.5 . So this shows that when n, the number of flips, is small the CLT doesn't hold for many values of p, so the Wald interval doesn't work very well.
  Figure: WaldFail.R
  FigureType: new

- Class: figure
  Output: Let's try the same experiment and increase n from 20 to 100 to see if the plot improves. Again, results may vary, but all the probabilities are much closer to the 95% line, so the CLT works better with a bigger value of n.
  Figure: WaldPass.R
  FigureType: new

- Class: text
  Output: A quick fix to the problem of having a small n is to use the Agresti/Coull interval. This simply means we add 2 successes and 2 failures to the counts when calculating the proportion p'. Specifically, if X is the number of successes out of the 20 coin flips, then instead of  setting p'=X/20, let p'=(X+2)/24. We use 24 as the number of trials since we've added 2 successes and 2 failures to the counts. Note that we still use 20 in the calculation of the upper and lower bounds.

- Class: figure
  Output: Here's a plot using this Agresti/Coull interval, with 1000 trials of 20 coin flips each. It looks much better than both the original Wald with 20 coin flips and the improved Wald with 100 coin flips. However, this technique might make the confidence interval too wide. 
  Figure: ACDemo.R
  FigureType: new

- Class: text
  Output: Why does this work? Adding 2 successes and 2 failures pulls p' closer to .5 which, as we saw, is the value which maximizes the confidence interval.

- Class: figure
  Output:  To show this simply, we wrote a function ACCompar, which takes an integer input n. For each k from 1 to n  it computes two fractions, k/n and (k+2)/(n+4). It then prints out the boolean vector of whether the new (k+2)/(n+4) fraction is less than the old k/n. It also prints out the total number of k's for which the the condition is TRUE. 
  Figure: ACComp.R
  FigureType: new

- Class: text
  Output: For all k less than  n/2, you see FALSE indicating that the new fraction is greater than or equal to k/n. For all k greater than n/2 you see TRUE indicating that the new fraction is less than the old. If k=n/2 the old and new fractions are equal.

- Class: cmd_question
  Output: Try running ACCompar now with an input of 20.
  CorrectAnswer: ACCompar(20)
  AnswerTests: omnitest(correctExpr=' ACCompar(20)')
  Hint: Type  ACCompar(20) at the command prompt.

- Class: text
  Output: Let's move on to Poisson distributions and confidence intervals. Recall that Poisson distributions model counts or rates. We write X~Poisson(lambda*t) where lambda is the expected count per unit of time and t is the total monitoring time.

- Class: text
  Output:  Suppose a nuclear pump failed 5 times out of 94.32 days and we want  a 95% confidence interval for the failure rate per day. The number of failures X is Poisson distributed with parameter (lambda*t). We estimate the failure rate, lambda', as x/t and the variance of our estimated mean is lambda/t, which we estimate as lambda'/t.

- Class: mult_question
  Output: In this example what would you use as the estimated mean x/t?
  AnswerChoices: 5/94.32; 94.32/5; I haven't a clue
  CorrectAnswer: 5/94.32
  AnswerTests: omnitest(correctVal='5/94.32')
  Hint: You need a number of failures divided by some measure of time.

- Class: cmd_question
  Output: Set a variable lamb now with this value.
  CorrectAnswer: lamb <- 5/94.32
  AnswerTests: omnitest(correctExpr='lamb <- 5/94.32')
  Hint: Type lamb <- 5/94.32 at the R prompt.

- Class: cmd_question
  Output: So lamb is our estimated mean and lamb/t is our estimated variance. The formula we've used to calculate a 95% confidence interval is est mean + c(-1,1)*qnorm(.975)*sqrt(est var). Use this formula now making the appropriate substitutions. 
  CorrectAnswer: lamb +c(-1,1)*qnorm(.975)*sqrt(lamb/94.32)
  AnswerTests: omnitest(correctExpr='lamb +c(-1,1)*qnorm(.975)*sqrt(lamb/94.32)')
  Hint: Type lamb +c(-1,1)*qnorm(.975)*sqrt(lamb/94.32) at the R prompt.

- Class: cmd_question
  Output: As a check we can use R's function poisson.test with the arguments 5 and 94.32 to check this result. This is an exact test so it guarantees coverage. As with the binomial exact test, we only need to look at the conf portion of the result using the x$conf construct. Do this now. 
  CorrectAnswer: poisson.test(5,94.32)$conf
  AnswerTests: omnitest(correctExpr='poisson.test(5,94.32)$conf')
  Hint: Type 'poisson.test(5,94.32)$conf' at the command prompt.

- Class: text
  Output: Pretty close, right? Now to check the coverage of our estimate we'll run the same simulation experiment we ran before with binomial distributions. We'll vary our lambda values from .005 to .1 with steps of .01 (so we have 10 of them), and for each one we'll generate 1000 Poisson samples with mean lambda*t. We'll calculate  sample means and use them to compute  95% confidence intervals. We'll then count how often out of the 1000 simulations the true mean (our lambda) was contained in the computed interval.
 
- Class: figure
  Output: Here's a plot of the results. We see that the coverage improves as lambda gets larger, and it's quite off for small lambda values.
  Figure: PoisDemo.R
  FigureType: new

- Class: figure
  Output: Now it's interesting to see how the coverage improves when we increase the unit of time. In the previous plot we used t=100 (rounding the 94.32 up). Here's a plot of the same experiment setting t=1000. We see that the coverage is much better for almost all the values of lambda, except for the smallest ones.
  Figure: PoisDemoImpr.R
  FigureType: new

- Class: text
  Output: Now for a quick review!

- Class: mult_question
  Output: What tells us that averages of iid samples converge to the population means that they are estimating?
  AnswerChoices: the law of small numbers; the law of large numbers; the CLT; the BLT
  CorrectAnswer: the law of large numbers
  AnswerTests: omnitest(correctVal='the law of large numbers')
  Hint: Think Big!

- Class: mult_question
  Output: What tells us that averages are approximately normal for large enough sample sizes
  AnswerChoices: the law of small numbers; the law of large numbers; the CLT; the BLT
  CorrectAnswer: the CLT
  AnswerTests: omnitest(correctVal='the CLT')
  Hint: Keep yourself centered!

- Class: mult_question
  Output: The Central Limit Theorem (CLT) tells us that averages have what kind of distributions?
  AnswerChoices: normal; abnormal; binomial; Poisson
  CorrectAnswer: normal
  AnswerTests: omnitest(correctVal='normal')
  Hint: Remember the previous question?

- Class: mult_question
  Output: The Central Limit Theorem (CLT) tells us that averages have normal distributions centered at what?
  AnswerChoices: the population mean; the population variance; the standard error
  CorrectAnswer: the population mean
  AnswerTests: omnitest(correctVal='the population mean')
  Hint: Remember the old E(X')=mu, where X' is the sample mean and mu is the population mean. Know what I mean?

- Class: mult_question
  Output: The Central Limit Theorem (CLT) tells us that averages have normal distributions with standard deviations equal to what?
  AnswerChoices: the population mean; the population variance; the standard error
  CorrectAnswer: the standard error
  AnswerTests: omnitest(correctVal='the standard error')
  Hint: Which choice has the word standard in it?

- Class: mult_question
  Output: True or False - The Central Limit Theorem (CLT) tells us that averages always have normal distributions no matter how big the sample size
  AnswerChoices: True; False
  CorrectAnswer: False
  AnswerTests: omnitest(correctVal='False')
  Hint:  Never trust statements with the words ALWAYS or NEVER in them. There are ALWAYS exceptions to rules. 

- Class: mult_question
  Output: To calculate a confidence interval for a mean you take the sample mean and add and subtract the relevant normal quantile times the what?
  AnswerChoices: standard error; variance; variance/n; mean
  CorrectAnswer: standard error
  AnswerTests: omnitest(correctVal='standard error')
  Hint: You want something like a standard deviation, right? Which choice has the word standard in it?

- Class: mult_question
  Output: For a 95% confidence interval approximately how many standard errors would you add and subtract from the sample mean? 
  AnswerChoices: 2; 4; 6; 8
  CorrectAnswer: 2
  AnswerTests: omnitest(correctVal='2')
  Hint: Anything above 3 is pretty far from the mean. Also, purists would prefer 1.96 for this.

- Class: mult_question
  Output: If you wanted increased coverage what would you do to your confidence interval?
  AnswerChoices: increase it; decrease it; keep it the same
  CorrectAnswer: increase it
  AnswerTests: omnitest(correctVal='increase it')
  Hint: The key word here is increase. Bigger interval means bigger coverage.

- Class: mult_question
  Output: If you had less variability in your data would your confidence interval get bigger or smaller?
  AnswerChoices: bigger; smaller
  CorrectAnswer: smaller
  AnswerTests: omnitest(correctVal='smaller')
  Hint: Recall the size of the confidence interval positively depends on standard error which is sqrt(var/n). If variance is smaller then so is variability and the interval.

- Class: mult_question
  Output: If you had larger sample size would your confidence interval get bigger or smaller?
  AnswerChoices: bigger; smaller
  CorrectAnswer: smaller
  AnswerTests: omnitest(correctVal='smaller')
  Hint: Recall the size of the confidence interval positively depends on standard error which is sqrt(var/n). If the sample size, n, gets bigger the standard error gets smaller and so does  the interval.

- Class: mult_question
  Output:  A quick fix for small sample size binomial calculations is what?
  AnswerChoices: add 2 successes and 2 failures; add 2 successes and 4 failures; add 2 successes and subtract 2 failures; changing data seem dishonest
  CorrectAnswer: add 2 successes and 2 failures
  AnswerTests: omnitest(correctVal='add 2 successes and 2 failures')
  Hint: Adding 2 successes and 2 failures brings the proportion of successes closer to 1/2 which maximizes the confidence interval.

- Class: text
  Output: Congrats! You've concluded this lesson on asymptotics. We hope you feel confident and are asymptomatic after going through it.

