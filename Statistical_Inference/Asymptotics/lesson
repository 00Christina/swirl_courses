- Class: meta
  Course: Statistical_Inference
  Lesson: Asymptotics
  Author: Swirl Coders
  Type: Coursera
  Organization: Johns Hopkins Bloomberg School of Public Health
  Version: 2.2.0

- Class: text
  Output: "Asymptotics. (Slides for this and other Data Science courses may be found at github https://github.com/DataScienceSpecialization/courses/. If you care to use them, they must be downloaded as a zip file and viewed locally. This lesson corresponds to 07_Statistical_Inference/Asymptopia.)"

- Class: text
  Output: In this lesson, we'll discuss asymptotics which describes how statistics behave as sample sizes get very large and approach infinity. Pretending sample sizes and populations are infinite is incredibly useful for simple statistical inference and approximations since it often lead to a nice understanding of procedures.

- Class: text
  Output: Asymptotics generally give no assurances about finite sample performance, but they form the basis for frequency interpretation of probabilities (the long run proportion of times an event occurs).

- Class: mult_question
  Output: Recall our simulations and discussions of sample means in previous lessons. We can now talk about the large sample distribution of sample means of a collection of iid observations. The mean of the sample mean estimates what?
  AnswerChoices: population mean; population variance; standard error; sigma^2/n
  CorrectAnswer: population mean
  AnswerTests: omnitest(correctVal='population mean')
  Hint: Which choice has the word 'mean' in it?

- Class: text
  Output: The Law of Large Numbers (LLN) says that the average approaches what it's estimating. We saw in our simulations that the larger the sample size the better the estimation.  As we flip a fair coin over and over, it eventually converges to the true probability of a head (.5). 

- Class: text
  Output: The LLN forms the basis of frequency style thinking. 

- Class: cmd_question
  Output: To see this in action, we've copied some code from the slides and created a function for you called cointPlot. It takes an integer, n, as a parameter which is the number of coin tosses you want to simulate. It then does this and computes the cumulative sum (assuming heads are 1 and tails 0), but as it performs each toss it divides the cumulative sum by the number of flips performed. It then plots this value for each of the k=1...n tosses. Try it now for n=10. 
  CorrectAnswer: coinPlot(10)
  AnswerTests: omnitest(correctExpr='coinPlot(10)')
  Hint: Type coinPlot(10) at the command prompt.

- Class: cmd_question
  Output: Your output depends on R's random number generator, but your plot probably jumps around a bit and, by the 10th flip, your cumulative sum/10 is probably different from mine. If you did this several times, your plots would vary quite a bit. Now call coinPlot again, this time with 10000 as the argument.
  CorrectAnswer: coinPlot(10000)
  AnswerTests: omnitest(correctExpr='coinPlot(10000)')
  Hint: Type coinPlot(10000) at the command prompt.

- Class: text
  Output: See the difference? Asymptotics in Action! The line approaches its asymptote of .5. This is the probability you expect since what we're plotting, the cumulative sum/number of flips, represents the probability of the coin landing on heads. As we know,  this is .5 .

- Class: text
  Output: We say that an estimator is CONSISTENT if it converges to what it's trying to estimate. The LLN says that the sample mean of iid samples is consistent for the population mean. This is good, right?

- Class: mult_question
  Output: Based on our previous lesson do you think the sample variance (and hence sample deviation) are consistent as well?
  AnswerChoices: Yes; No
  CorrectAnswer: Yes
  AnswerTests: omnitest(correctVal='Yes')
  Hint: Recall our simulations of sample variances and how, as we increased the sample size, they converged to the population variance.

- Class: text
  Output: Now for something really important - the CENTRAL LIMIT THEOREM (CLT) - one of the most important in all of statistics. It states that the distribution of averages of iid variables (properly normalized) becomes that of a standard normal as the sample size increases. 
 
- Class: text
  Output: Let's dissect this to see what it means. First, 'properly normalized' means that you subtract the population mean mu from the sample mean X' and divided this difference by sigma/sqrt(n). Here sigma is the standard deviation of the population and n is the sample size. 

- Class: text
  Output: Second, this normalized variable, (X'-mu)/(sigma/sqrt(n)) is a random variable from a normal distribution with mean 0 and variance 1. Remember that n must be large for the CLT to apply.

- Class: mult_question
  Output: Do you recognize sigma/sqrt(n) from our lesson on variance? It is the often approximated by what? 
  AnswerChoices: the standard error of the sample mean; the variance of the population; the mean of the population; I give up
  CorrectAnswer: standard error of the sample mean
  AnswerTests: omnitest(correctVal='standard error of the sample mean')
  Hint: Recall all our simulation experiments in the variance lesson where we calculated standard deviations of means using R's sd function, then we calculated an approximation using a formula involving the population variance and the square root of the sample size.

- Class: text
  Output: Let's repeat this in an equivalent but different way. Suppose X_1, X_2, ... X_n are independent random variables from an infinite population with mean mu and variance sigma^2. Then if n is large, the mean of the X's, call it X', is approximately normal with mean mu and variance sigma^2/n. We denote this as N(mu,sigma^2/n).

- Class: figure
  Output: To show the CLT in action consider this figure from the slides. It presents the histogram of 1000 averages of dice rolls with sample sizes of 10, 20 and 30 respectively. Each average of n dice rolls (n=10,20,30) has been normalized by subtracting off the mean (3.5) then dividing by the standard error, sqrt(2.92/n). Recall that for a dice roll the mean is 3.5 and variance is 2.92. The normalization has made each histogram look like a standard normal, i.e., mean 0 and standard deviation 1.
  Figure: cltDice.R
  FigureType: new

- Class: text
  Output: Notice that the CLT said nothing about the original population being normally distributed. That's precisely where its usefulness lies! We can assume normality of a population mean no matter what kind of population we have, as long as our sample size is large enough. Let's look at how it works with a binomial experiment like flipping a coin.

- Class: text
  Output: Recall that if the probability of a head (call it 1) is p, then the probability of a tail (0) is 1-p. The expected value then is p and the variance is p-p^2 or p(1-p). Suppose we do n coin flips and let p' represent the average of these n flips. We normalize p' by subtracting p and dividing by sqrt(p(1-p)/n). Let's do this for 1000 trials and plot the resulting histogram. 

- Class: figure
  Output: Here's a figure from the slides showing the results of 3 such trials where each trial is for a different value of n (10, 20, and 30) and the coin is fair,so E(X)=.5 and the standard error is 1/(2sqrt(n)). Notice how with larger n the histogram tightens up around the mean 0.
  Figure: cltFairCoin.R
  FigureType: new

- Class: figure
  Output: Here's another plot from the slides of the same experiment, this time using a biassed coin. We set the probability of a head to .9, so E(X)=.9 and the standard error is sqrt(.09/n) Again, the larger the sample size the more closely the distribution looks normal, although with this biassed coin the normal approximation isn't as good as it was with the fair coin.
  Figure: cltUnfairCoin.R
  FigureType: new

- Class: text
  Output: Now let's talk about confidence intervals.

- Class: figure
  Output:  We know from the CLT that for large n, the sample mean is normal with mean mu and standard deviation sigma/sqrt(n). We also know that 95% of the area under a normal curve is within two standard deviations of the mean.This figure illustrates this point; the entire shaded portion depicts the area within 2 standard deviations of the mean and the darker portion shows the 68% of the area within 1 standard deviation. 
  Figure: stddev1.R
  FigureType: new

- Class: text
  Output: It follows that 5% of the area under the curve is not shaded. By symmetry of the curve, only 2.5% of the data is greater than the mean + 2 standard deviations (mu+2*sigma/sqrt(n)) and only 2.5% is less than the  mean - 2 standard deviations (mu-2*sigma/sqrt(n)). 

- Class: text
  Output: So the probability that the sample mean X' is bigger than mu + 2sigma/sqrt(n) OR smaller than mu-2sigma/sqrt(n) is 5%.  Equivalently, the probability of being between these limits is 95%. 

- Class: text
  Output: The quantity X' + or - 2 sigma/sqrt(n) is called a 95% interval for mu. The 95% says that if one were to repeatly get samples of size n, about 95% of the intervals obtained would contain mu, the quantity we're trying to estimate. 

- Class: mult_question
  Output: Note that for a 95% confidence interval we divide (100-95) by 2 (since we have both left and right tails) and add the result to 95 to compute the quantile we need. The 97.5 quantile is actually 1.96, but for simplicity it's often just rounded up to 2. If you wanted to find a 90% confidence interval what quantile would you want?
  AnswerChoices: 90; 95; 85; 100
  CorrectAnswer: 95
  AnswerTests: omnitest(correctVal='95')
  Hint: Divide (100-90) by 2 and add this result to 90.

- Class: cmd_question
  Output: Use the R function qnorm to find the 95 quantile for a standard normal distribution. Remember that qnorm takes a probability as an input. You can use default values for all the other arguments.
  CorrectAnswer: qnorm(.95)
  AnswerTests: omnitest(correctExpr='qnorm(.95)')
  Hint: Type qnorm(.95) at the command prompt.

- Class: text
 Output: As we've seen before, in a binomial distribution in which p represents the probability or proportion of success, the variance sigma^2 is p(1-p), so the standard error of the sample mean p' is sqrt(p(1-p)/n) where n is the sample size. The 95% confidence interval of p is then p' +/- 2*sqrt(p(1-p)/n). (As before, we use 2 as the 97.5 quantile.) We don't know the true value of p; that's what we're trying to estimate. The Wald confidence interval replaces p with p'.

- Class: text
   Output:  Since we don't know p, we want to maximize the 95% confidence interval represented by 2*sqrt(p(1-p)/n). From calculus we know that p(1-p) is maximized when p=1/2, so our biggest 95% confidence interval estimate would use 1/2 for a value of p in the formula p'+/- 2*sqrt(p(1-p)/n). 1/sqrt(n).

- Class: mult_question
  Output: Using 1/2 for the value of p in the formula yields what 95% confidence interval for p?
  AnswerChoices: p'+/- 1/sqrt(n); p'+/- 1/2sqrt(n); p'+/ 2sqrt(n)
  CorrectAnswer: p'+/- 1/sqrt(n)
  AnswerTests: omnitest(correctVal='p'+/- 1/sqrt(n)')
  Hint: p(1-p)=1/4 when p=1/2 and the sqrt(1/4n)=1/(2sqrt(n)). What happens when you multiply this by 2?

- Class: mult_question
  Output: We'll illustrate how to use this formula with an example from the slides. Suppose you were running for office and your pollster polled 100 people. Of these 60 claimed they were going to vote for you. You'd like to see if the true proportion of people who will vote for you and you want to be 95% confident of your estimate. We need to find the limits that will contain the true proportion of your supporters with 95% confidence, so we'll use the formula p' +/- 1/sqrt(n) to answer this question. First, what value would you use for p', the sampled estimate?
  AnswerChoices: 60; 56; 100; 10
  CorrectAnswer: 60
  AnswerTests: omnitest(correctVal='60')
  Hint: The only sampled number here is the number of people who said they would vote for you.

- Class: mult_question
  Output: What would you use for 1/sqrt(n)?
  AnswerChoices: 1/sqrt(60); 1/sqrt(56); 1/100; 1/10
  CorrectAnswer: 1/10
  AnswerTests: omnitest(correctVal='1/10')
  Hint: The sample size is n, and in this case n=100. What is 1/sqrt(100)?

- Class: mult_question
  Output: The bounds of the interval then are what?
  AnswerChoices: .5 and .7; .46 and .66; .55 and .65; I haven't a clue
  CorrectAnswer: .5 and .7
  AnswerTests: omnitest(correctVal='.5 and .7')
  Hint: We know p'- 1/sqrt(n) is the lower bound and p'+ 1/sqrt(n) is the upper bound, so use the answers from the two previous answers to fill in values for these variables.

- Class: mult_question
  Output: How do you feel about the election?
  AnswerChoices: confident; unsure; I'll pull out;  Perseverance, that's the answer; 
  CorrectAnswer: confident
  AnswerTests: omnitest(correctVal='confident')
  Hint: With 95% confidence, between .5 and .7 of the voters support you. You look like a winner to me!

- Class: cmd_question
  Output: To reassure you that this is a pretty good estimate, we can use R to get a more precise confidence interval using the actual quantile 1.96 instead of our ballpark estimate of 2. Use the formula  p'+/- qnorm(.975)*sqrt(p'(1-p')/100). Use the p' and n values from above and the R construct p'+c(-1,1)... to handle the plus/minus portion of the formula. You should see bounds similar to the ones you just estimated.
  CorrectAnswer: .6 + c(-1,1)*qnorm(.975)*sqrt(.6*.4/100)
  AnswerTests: omnitest(correctExpr='.6 + c(-1,1)*qnorm(.975)*sqrt(.6*.4/100)')
  Hint: Type .6 + c(-1,1)*qnorm(.975)*sqrt(.6*.4/100) at the command prompt. 

- Class: cmd_question
  Output: As an alternative to this Wald interval, we can also use the R function binom.test with the parameters 60 and 100 and let all the others default. This function "performs an exact test of a simple null hypothesis about the probability of success in a Bernoulli experiment." (This means it uses a lot of computation and doesn't rely on the CLT.) It returns a lot of information but all we want to look at now is the values of the confidence interval, conf.int, that it returns. Use the R construct x$conf.int to find these now.
  CorrectAnswer: binom.test(60,100)$conf.int
  AnswerTests: omnitest(correctExpr='binom.test(60,100)$conf.int')
  Hint: Type binom.test(60,100)$conf.int at the command prompt. 

- Class: text
  Output: Now we're going to see that the Wald interval isn't very accurate when n is small. We'll use the example from the slides. 

- Class: figure
  Output: Suppose we flip a coin a small number of times, say 20. Also suppose we have a function mywald which takes a probability p, and generates 30 sets of 20 coin flips using that probability p. It uses the sampled proportion of success, p', for those 20 coin flips to compute the upper and lower bounds of the 95% Wald interval, that is, it computes the two numbers p'+/- qnorm(.975) * sqrt(p' * (1-p') / n) for each of the 30 trials. For the given true probability p, we  count the number of times out of those 30 trials that the true probability p was in the Wald confidence interval. We'll call this the coverage.
  Figure: WaldDemo.R
  FigureType: new

- Class: cmd_question
  Output: To make sure you understand what's going on, trying running mywald now with the probability .2. It will print out 30 p' values (which you don't really need to see), followed by 30 lower bounds, 30 upper bounds and lastly the percentage of times that the input .2 was between the two bounds. See if you agree with the percentage you get. Usually it suffices to just count the number of times (out of the 30) that .2 is less than the upper bound.
  CorrectAnswer: mywald(.2)
  AnswerTests: omnitest(correctExpr='mywald(.2)')
  Hint: Type mywald(.2) at the command prompt. 

- Class: text
  Output: Now that you understand the underlying principle, suppose instead of counting the number of times the true (input) probability fell within the 95% Wald interval out of 30 trials, we used 1000 trials. Also suppose we did this experiment for a series of probabilities, say from .1 to .9 taking steps of size .05. More specifically, we call our function using 17 different probabilities, namely .1, .15, .2, .25, ... .9 . We can then plot the percentages of coverage for each of the probabilities. 

- Class: figure
  Output: Here's the plot of our results. Each of the 17 vertices show the percentage of coverage for a particular true probability p passed to the function. Results will vary, but usually the only probability that hits above the 95% line is the p=.5 . So this shows that when n, the number of flips, is small the CLT doesn't hold for many values of p, so the Wald interval doesn't work very well.
  Figure: WaldFail.R
  FigureType: new

- Class: figure
  Output: Let's try the same experiment and increase n from 20 to 100 to see if the plot improves. Again, results may vary, but all the probabilities are much closer to the 95% line, so the CLT works better with a bigger value of n.
  Figure: WaldPass.R
  FigureType: new

- Class: text
  Output: A quick fix to the small n problem is to use the Agresti/Coull interval. This simply means we add 2 successes and 2 failures to the counts when calculating p'. Specifically, if X is the number of successes out of the 20 coin flips, then instead of  p'=X/20, let p'=(X+2)/24. We use 24 as the number of trials since we've added 2 successes and 2 failures to the counts. Note that we still use 20 in the calculation of the upper and lower bounds (in the standard error portion of the formula).

- Class: figure
  Output: Here's a plot using this Agressi/Coull interval, with 1000 trials of 20 coin flips each. It looks much better than both the original Wald with 20 coin flips and the improved Wald with 100 trials.
  Figure: ACDemo.R
  FigureType: new

- Class: text
  Output: Let's move on to Poisson distributions and confidence intervals.

- Class: cmd_question
  Output: Now we need weights. For these we can use any of the three PDF's, (dice_fair, dice_high, and dice_low) we defined in the previous lesson. Using R's ability to multiply vectors componentwise and its function 'sum' we can easily compute E(X^2) for any of these dice. Simply sum the product dice_sqr * PDF.  Try this now with dice_fair and put the result in a variable ex2_fair.
  CorrectAnswer: ex2_fair <- sum(dice_fair * dice_sqr)
  AnswerTests: ANY_of_exprs('ex2_fair <- sum(dice_fair * dice_sqr)','ex2_fair <- sum(dice_sqr * dice_fair)')
  Hint: Type 'ex2_fair <- sum(dice_fair * dice_sqr)' at the command prompt.

- Class: cmd_question
  Output: Recall that the expected value of a fair dice roll is 3.5. Subtract the square of that from ex2_fair to compute the sample variance.
  CorrectAnswer: ex2_fair-3.5^2
  AnswerTests: ANY_of_exprs('ex2_fair-3.5^2','ex2_fair-3.5*3.5')
  Hint: Type 'ex2_fair-3.5^2' at the command prompt.

- Class: cmd_question
  Output: Now use a similar approach to compute the sample variance of dice_high in one step. Sum the appropriate product and subtract the square of the mean. Recall that edh holds the expected value of dice_high.
  CorrectAnswer: sum(dice_high * dice_sqr)-edh^2
  AnswerTests: ANY_of_exprs('sum(dice_high * dice_sqr)-edh^2','sum(dice_sqr * dice_high)-edh^2','sum(dice_high * dice_sqr)-edh*edh','sum(dice_sqr * dice_high)-edh*edh')
  Hint: Type 'sum(dice_high * dice_sqr)-edh^2' at the command prompt.

- Class: text
  Output: Note that when we talk about variance we're using square units. Because it is often more useful to use measurements in the same units as X we define the standard deviation of X as the square root of Var(X). 

- Class: figure
  Output: Here's a figure from the slides. It shows several normal distributions all centered around a common mean 0, but with different standard deviations. As you can see from the color key on the right, the thinner the bell the smaller the standard deviation and the bigger the standard deviation the fatter the bell.
  Figure: normalVar.R
  FigureType: new

- Class: text
  Output: Just as we distinguished between a population mean and a sample mean we have to distinguish between a population variance sigma^2 and a sample variance s^2. They are defined similarly but with a slight difference. The sample variance is defined as the sum of n squared distances from the sample mean divided by (n-1), where n is the number of samples. We divide by n-1 because this is the number of degrees of freedom in the system. The first n-1 samples are independent but the last isn't since it can be calculated from the sample mean we're using in the formula.

- Class: text
  Output:  In other words, the sample variance is ALMOST the average squared deviation from the sample mean.

- Class: text
  Output: As with the sample mean, the sample variance is also a random variable with an associated population distribution. Its expected value or mean is the population variance and its distribution gets more concentrated around the population variance with more data. The sample standard deviation is the square root of the sample variance.

- Class: figure
  Output: To illustrate this point, consider this figure which plots the distribution of 10000 variances, Each variance was computed on sample of standard normals of size 10. The vertical line indicates the standard deviation 1.
  Figure: moreData1.R
  FigureType: new

- Class: figure
  Output: Here we do the same experiment but this time each of the 10000 variances is over 20 standard normal samples. We've plotted over the first plot and you can see that the distribution of the variances is getting tighter and shifting closer to the vertical line.
  Figure: moreData2.R
  FigureType: new

- Class: figure
  Output: Finally, we repeat the experiment using 30 samples for each of the 10000 variances. You can see that with more data, the distribution gets more concentrated around the population variance it is trying to estimate. 
  Figure: moreData3.R
  FigureType: new

- Class: text
  Output: Now recall that unbiased estimators equal the values they're trying to estimate. We can infer  from the above that the sample variance is an unbiased estimator of population variance. 

- Class: text
  Output: Recall that the average of random samples from a population is itself a random variable with a distribution centered around the population mean. Specifically, E(X') = mu, where X' represents a sample mean and mu is the population mean.

- Class: text
  Output:  We can show that, if the population is infinite, the variance of the sample mean is the population variance divided by the sample size. Specifically,  Var(X') = sigma^2 / n. Let's work through this in four short steps.

- Class: mult_question
  Output: Which of the following does Var(X') equal? Here X' represents the sample mean and 'Sum(X_i)' represents the sum of the n samples X_1,...X_n.
  AnswerChoices: Var(1/n * Sum(X_i)); E(1/n * Sum(X_i)); mu; sigma
  CorrectAnswer: Var(1/n * Sum(X_i))
  AnswerTests: omnitest(correctVal='Var(1/n * Sum(X_i))')
  Hint: Which of the choices has both Var and the definistion of mean in it?

- Class: mult_question
  Output: Which of the following does Var(1/n * Sum(X_i)') equal?
  AnswerChoices: 1/n^2*Var(Sum(X_i)); 1/n^2*E(Sum(X_i)); mu/n^2; sigma/n
  CorrectAnswer: 1/n^2*Var(Sum(X_i))
  AnswerTests: omnitest(correctVal='1/n^2*Var(Sum(X_i))')
  Hint: Remember that fact about Var that we said would be useful before? Now is the time to use it.

- Class: mult_question
  Output: Recall that Var is an expected value and expected values are linear. What does Var(Sum(X_i)) equal?
  AnswerChoices: Sum(Var(X_i)); E(Sum(X_i)); E(mu); Var(sigma)
  CorrectAnswer: Sum(Var(X_i))
  AnswerTests: omnitest(correctVal='Sum(Var(X_i))')
  Hint: By linearity, the variance of the sum equals the sum of the variance.

- Class: mult_question
  Output: Finally, each X_i comes from a population with variance sigma^2. What does Sum(Var(X_i)) equal? As before, Sum is taken over n values.
  AnswerChoices: n*(sigma)^2; n*mu; n*E(mu); (n^2)*Var(sigma)
  CorrectAnswer: n*(sigma)^2
  AnswerTests: omnitest(correctVal='n*(sigma)^2')
  Hint: Var(X_i) is the constant value sigma^2 and we're summing over n of them.

- Class: text
  Output: So we shown Var(X')=Var(1/n*Sum(X_i))=(1/n^2)*Var(Sum(X_i))=(1/n^2)*Sum(sigm^2)=sigma^2/n for infinite populations. 

- Class: text
  Output: The standard deviation of a statistic is called its standard error, so the standard error of the sample mean is the square root of its variance.

- Class: text
  Output: We just showed that the variance of sample mean is sigma^2 / n and we estimate it with s^2 / n. It follows that its square root, s / sqrt(n), is the standard error of the sample mean.

- Class: text
  Output: The sample standard deviation, s, tells us how variable the population is, and s/sqrt(n), the standard error, tells us how much averages of random samples of size n from the population vary. Let's see this with some simulations.

- Class: cmd_question
  Output: The R function rnorm(n,mean,sd) generates n random normal numbers with the specified mean and standard deviation. The defaults for the latter are mean 0 and standard deviation 1. Type the expression sd(apply(matrix(rnorm(10000),1000),1,mean)) at the prompt. 
  CorrectAnswer: sd(apply(matrix(rnorm(10000),1000),1,mean))
  AnswerTests: omnitest(correctExpr='sd(apply(matrix(rnorm(10000),1000),1,mean))')
  Hint: Type 'sd(apply(matrix(rnorm(10000),1000),1,mean))' at the command prompt.

- Class: cmd_question
  Output: This returns the standard deviation of 1000 averages, each of a sample of 10 random normal numbers with mean 0 and standard deviation 1. The theory tells us that the standard error, s/sqrt(n), of the sample means indicates how much averages of random samples of size n (in this case 10) vary. Now compute 1/sqrt(10) to see if it matches the standard deviation we just computed with our simulation.
  CorrectAnswer: 1/sqrt(10)
  AnswerTests: omnitest(correctExpr='1/sqrt(10)')
  Hint: Type '1/sqrt(10)' at the command prompt.

- Class: mult_question
  Output: Pretty close, right? Let's try a few more. Standard uniform distributions have variance 1/12. The theory tells us the standard error of means of samples of size n would have which standard error?
  AnswerChoices: 1/(12*sqrt(n)); 12/sqrt(n); 1/sqrt(12*n); I haven't a clue 
  CorrectAnswer: 1/sqrt(12*n)
  AnswerTests: omnitest(correctVal='1/sqrt(12*n)')
  Hint: In this case s is the sqrt(1/12). Divide this by sqrt(n).

- Class: cmd_question
  Output:  Compute 1/sqrt(120). This would be the standard error of the means of uniform samples of size 10. 
  CorrectAnswer: 1/sqrt(120)
  AnswerTests: omnitest(correctExpr='1/sqrt(120)')
  Hint: Type '1/sqrt(120)' at the command prompt.

- Class: cmd_question
  Output: Now check it as we did before. Use the expression sd(apply(matrix(runif(10000),1000),1,mean)). 
  CorrectAnswer: sd(apply(matrix(runif(10000),1000),1,mean))
  AnswerTests: omnitest(correctExpr='sd(apply(matrix(runif(10000),1000),1,mean))')
  Hint: Type 'sd(apply(matrix(runif(10000),1000),1,mean))' at the command prompt.

- Class: mult_question
  Output: Pretty close again, right? Poisson(4) are distributions with variance 4; what standard error would means of random samples of n Poisson(4) have?
  AnswerChoices: 2/sqrt(n); 1/sqrt(2*n); 2*sqrt(n); I haven't a clue 
  CorrectAnswer:  2/sqrt(n)
  AnswerTests: omnitest(correctVal='2/sqrt(n)')
  Hint: In this case s is 2. Divide this by sqrt(n).

- Class: cmd_question
  Output:  We'll do another simulation to test the theory. First, assume you're taking averages of 10 Poisson(4) samples and compute the standard error of these means. Use the formula you just chose.
  CorrectAnswer: 2/sqrt(10)
  AnswerTests: omnitest(correctExpr='2/sqrt(10)')
  Hint: Type '2/sqrt(10)' at the command prompt.

- Class: cmd_question
  Output: Now check it as we did before. Use the expression sd(apply(matrix(rpois(10000,4),1000),1,mean)). 
  CorrectAnswer: sd(apply(matrix(rpois(10000,4),1000),1,mean))
  AnswerTests: omnitest(correctExpr='sd(apply(matrix(rpois(10000,4),1000),1,mean))')
  Hint: Type 'sd(apply(matrix(rpois(10000,4),1000),1,mean))' at the command prompt.

- Class: mult_question
  Output: Like magic, right? One final test. Fair coin flips have variance 0.25; means of random samples of n coin flips have  what standard error?
  AnswerChoices: 2/sqrt(n); 1/sqrt(2*n); 2*sqrt(n); 1/(2*sqrt(n)); I haven't a clue 
  CorrectAnswer:  1/(2*sqrt(n))
  AnswerTests: omnitest(correctVal='1/(2*sqrt(n))')
  Hint: In this case s is 1/2 which is the sqrt of 1/4, the variance. Divide this by sqrt(n).

- Class: cmd_question
  Output:  You know the drill. Assume you're taking averages of 10 coin flips and compute the standard error of these means with the theoretical formula you just picked.
  CorrectAnswer: 1/(2*sqrt(10))
  AnswerTests: omnitest(correctExpr=' 1/(2*sqrt(10))')
  Hint: Type ' 1/(2*sqrt(10))' at the command prompt.

- Class: cmd_question
  Output: Now check it as we did before. Use the expression sd(apply(matrix(sample(0:1,10000,TRUE),1000),1,mean)). 
  CorrectAnswer: sd(apply(matrix(sample(0:1,10000,TRUE),1000),1,mean))
  AnswerTests: omnitest(correctExpr='sd(apply(matrix(sample(0:1,10000,TRUE),1000),1,mean))')
  Hint: Type 'sd(apply(matrix(sample(0:1,10000,TRUE),1000),1,mean))' at the command prompt.

- Class: text
  Output: Finally, here's something interesting. Chebyshev's inequality helps interpret variances. It states that the probability that a random variable X is at least k standard deviations from its mean is less than 1/(k^2). In other words, the probability that X is at least 2 standard deviations from the mean is less than 1/4, 3 standard deviations 1/9, 4 standard deviations 1/16, etc.

- Class: text
  Output: However this estimate is quite conservative for random variables that are normally distributed, that is, with bell-curve distributions. In these cases, the probability of being at least 2 standard deviations from the mean is about 5% (as compared to Chebyshev's 25%) and the probability of being at least 3 standard deviations from the mean is roughly .2%.

- Class: mult_question
  Output: Suppose you had a measurement that was 4 standard deviations from the the distribution's mean. What would be the upper bound  of the probability of this happening using Chebyshev's inequality? 
  AnswerChoices: 6%; 0%; 11%; 25%; 96% 
  CorrectAnswer: 6%
  AnswerTests: omnitest(correctVal='6%')
  Hint: Chebyshev's inequality estimates that probability as 1/16. Convert this to a probability.


- Class: mult_question
  Output: Now to review. The sample variance estimates what?
  AnswerChoices: population variance; sample mean; sample standard deviation; population
  CorrectAnswer: population variance
  AnswerTests: omnitest(correctVal='population variance')
  Hint: Which choice has the word variance in it?

- Class: mult_question
  Output: The distribution of the sample variance is centered at what?
  AnswerChoices: population variance; sample mean; sample standard deviation; population
  CorrectAnswer: population variance
  AnswerTests: omnitest(correctVal='population variance')
  Hint: What is the sample variance estimating?

- Class: mult_question
  Output: True or False - The sample variance gets more concentrated around the population variance with larger sample sizes
  AnswerChoices: True; False
  CorrectAnswer: True
  AnswerTests: omnitest(correctVal='True')
  Hint: Is more data better than less data?

- Class: mult_question
  Output: The variance of the sample mean is the population variance divided by ?
  AnswerChoices: n; n^2; sqrt(n); I haven't a clue
  CorrectAnswer: n
  AnswerTests: omnitest(correctVal='n')
  Hint: Remember the 4 step proof starting with Var(X')=...? The last step had an n divided by an n^2.

- Class: mult_question
  Output: The standard error of the sample mean is the sample standard deviation s divided by ?
  AnswerChoices: n; n^2; sqrt(n); I haven't a clue
  CorrectAnswer: sqrt(n)
  AnswerTests: omnitest(correctVal='sqrt(n)')
  Hint: Remember the many many examples we went through. The sqrt(n) figured prominently in them.

- Class: text
  Output: Congrats! You've concluded this vary long lesson on variance. We hope you liked it vary much.

